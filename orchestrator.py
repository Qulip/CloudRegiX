from typing import Dict, Any, List, Generator
import time
import asyncio

from agents import (
    RouterAgent,
    PlannerAgent,
    AnswerAgent,
    ReActExecutorAgent,
    TraceManagerAgent,
)
from tools import (
    ReasoningTraceLogger,
    PlanRevisionTool,
    StateManager,
    SlideFormatterTool,
)
from mcp_client import get_mcp_client

# langchain-mcp-adaptersë¥¼ ì‚¬ìš©í•œ MCP ë„êµ¬ ë¡œë”©
from langchain_mcp_adapters.client import MultiServerMCPClient


class CloudGovernanceOrchestrator:
    """
    í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ AI ì‹œìŠ¤í…œ í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
    Plan & Execute + ReAct í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬
    """

    def __init__(self):
        self.router_agent = RouterAgent()
        self.planner_agent = PlannerAgent()
        self.answer_agent = AnswerAgent()

        # ê¸°ì¡´ MCP í´ë¼ì´ì–¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)
        self.mcp_client = get_mcp_client()

        # LangChain Tool ì§ì ‘ ì‚¬ìš©
        self.slide_formatter = SlideFormatterTool()

        # ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì„± ìš”ì†Œë“¤
        self.trace_manager = TraceManagerAgent()
        self.reasoning_trace_logger = ReasoningTraceLogger()
        self.plan_revision_tool = PlanRevisionTool()
        self.state_manager = StateManager()

        # ReAct Executor Pool
        self.executor_pool = {}
        self.max_executors = 5

        # MCP ë„êµ¬ë“¤ì„ ìœ„í•œ MultiServerMCPClient ì„¤ì •
        self.mcp_multi_client = None
        self.mcp_tools = []
        self._initialize_mcp_tools()

        self.mcp_context = {
            "role": "hybrid_orchestrator",
            "function": "hybrid_workflow_coordination",
            "agents_initialized": True,
            "hybrid_mode": True,
            "mcp_tools_available": True,
            "langchain_tools_available": True,
        }

    def _initialize_mcp_tools(self):
        """MCP ë„êµ¬ë“¤ì„ ì´ˆê¸°í™”"""
        try:
            # MultiServerMCPClient ì„¤ì •
            self.mcp_multi_client = MultiServerMCPClient(
                {
                    "cloud_governance": {
                        "url": "http://localhost:8001/tools",
                        "transport": "streamable_http",
                    }
                }
            )
            print("âœ… MCP MultiServerMCPClient ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ MCP ë„êµ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.mcp_multi_client = None

    async def _get_mcp_tools(self):
        """MCP ë„êµ¬ë“¤ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if self.mcp_multi_client:
                tools = await self.mcp_multi_client.get_tools()
                return tools
            return []
        except Exception as e:
            print(f"âš ï¸ MCP ë„êµ¬ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            return []

    def _run_async_mcp_operation(self, coro):
        """ë¹„ë™ê¸° MCP ì‘ì—…ì„ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰"""
        try:
            loop = asyncio.get_running_loop()
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            import concurrent.futures
            import threading

            def run_in_thread():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(coro)
                finally:
                    new_loop.close()

            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_in_thread)
                return future.result()

        except RuntimeError:
            # ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(coro)
            finally:
                loop.close()

    def process_request(self, user_input: str) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ

        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥

        Returns:
            Dict[str, Any]: ìµœì¢… ì‘ë‹µ
        """
        start_time = time.time()

        try:
            print(f"ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì‹œì‘: {user_input[:50]}...")

            # 1ë‹¨ê³„: Router Agent - ì˜ë„ ë¶„ì„
            print("\nğŸ“ 1ë‹¨ê³„: Router Agent - ì˜ë„ ë¶„ì„")
            router_result = self.router_agent({"user_input": user_input})
            print(f"   â”” Intent: {router_result.get('intent', 'unknown')}")

            # 2ë‹¨ê³„: Enhanced Planner Agent - í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
            print("\nğŸ“‹ 2ë‹¨ê³„: Enhanced Planner - í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½")
            planner_input = {**router_result, "user_input": user_input}
            plan_result = self.planner_agent(planner_input)

            return self._process_hybrid_execution(
                plan_result, router_result, user_input, start_time
            )

        except Exception as e:
            error_time = time.time() - start_time
            print(f"\nâŒ í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì˜¤ë¥˜: {str(e)}")
            return self._create_error_response(str(e), error_time)

    def process_request_streaming(
        self, user_input: str
    ) -> Generator[Dict[str, Any], None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ

        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥

        Yields:
            Dict[str, Any]: ìŠ¤íŠ¸ë¦¬ë° ì²­í¬
        """
        start_time = time.time()

        try:
            print(f"\nğŸš€ [ORCHESTRATOR] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘: {user_input[:50]}...")

            yield {
                "type": "progress",
                "stage": "router_analysis",
                "message": "ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "progress": 0.1,
            }

            # 1ë‹¨ê³„: Router Agent - ì˜ë„ ë¶„ì„
            print(f"\nğŸ“ [STEP 1] Router Agent ì‹¤í–‰ ì¤‘...")
            router_result = self.router_agent({"user_input": user_input})
            intent = router_result.get("intent", "unknown")
            print(f"   âœ… [ROUTER] ì˜ë„ ë¶„ì„ ì™„ë£Œ: {intent}")
            print(f"   ğŸ“Š [ROUTER] ì „ì²´ ê²°ê³¼: {router_result}")

            yield {
                "type": "progress",
                "stage": "planner_analysis",
                "message": f"ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ìˆìŠµë‹ˆë‹¤... (ì˜ë„: {intent})",
                "progress": 0.2,
                "intent": intent,
            }

            # 2ë‹¨ê³„: Enhanced Planner Agent - í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
            print(f"\nğŸ“‹ [STEP 2] Planner Agent ì‹¤í–‰ ì¤‘...")
            planner_input = {**router_result, "user_input": user_input}
            print(f"   ğŸ“¥ [PLANNER] ì…ë ¥ ë°ì´í„°: {planner_input}")
            plan_result = self.planner_agent(planner_input)
            print(f"   âœ… [PLANNER] ê³„íš ìˆ˜ë¦½ ì™„ë£Œ")
            print(f"   ğŸ“Š [PLANNER] ì „ì²´ ê²°ê³¼: {plan_result}")

            execution_steps = plan_result.get("execution_steps", [])
            dependency_graph = plan_result.get("dependency_graph", {})

            print(f"   ğŸ“‹ [PLANNER] ì‹¤í–‰ ë‹¨ê³„ ìˆ˜: {len(execution_steps)}")
            for i, step in enumerate(execution_steps):
                print(
                    f"      Step {i+1}: {step.get('step_id', 'unknown')} - {step.get('description', 'No description')[:50]}..."
                )

            yield {
                "type": "progress",
                "stage": "execution_start",
                "message": f"{len(execution_steps)}ê°œ ë‹¨ê³„ì˜ ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...",
                "progress": 0.3,
                "steps_count": len(execution_steps),
            }

            # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)
            print(f"\nâš¡ [STEP 3] í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ì‹œì‘ ({len(execution_steps)}ê°œ ë‹¨ê³„)")
            execution_context = {
                "user_input": user_input,
                "intent": router_result.get("intent"),
                "key_entities": router_result.get("key_entities", []),
                "execution_steps": execution_steps,
                "execution_plan": execution_steps,
                "dependency_graph": dependency_graph,
            }

            # ë‹¨ê³„ë³„ ì‹¤í–‰ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬
            execution_results = []
            for i, step in enumerate(execution_steps):
                step_progress = 0.3 + (0.5 * (i + 1) / len(execution_steps))
                step_id = step.get("step_id", f"step_{i+1}")
                step_description = step.get("description", "Unknown step")
                required_tools = step.get("required_tools", [])

                print(f"\n   ğŸ”„ [STEP 3.{i+1}] ë‹¨ê³„ ì‹¤í–‰ ì‹œì‘: {step_id}")
                print(f"      ğŸ“ ì„¤ëª…: {step_description}")
                print(f"      ğŸ› ï¸  í•„ìš” ë„êµ¬: {required_tools}")

                yield {
                    "type": "progress",
                    "stage": "step_execution",
                    "message": f"ë‹¨ê³„ {i+1}/{len(execution_steps)} ì‹¤í–‰ ì¤‘: {step_description}",
                    "progress": step_progress,
                    "current_step": step_id,
                }

                try:
                    # ë‹¨ê³„ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
                    print(f"      ğŸ¯ [EXECUTION] ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì‹œë„...")
                    step_result = self._execute_step_streaming(step, execution_context)

                    if step_result:
                        print(f"      âœ… [EXECUTION] ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì„±ê³µ")
                        final_result = None
                        chunk_count = 0

                        for chunk in step_result:
                            chunk_count += 1
                            print(
                                f"         ğŸ“¦ [CHUNK {chunk_count}] íƒ€ì…: {chunk.get('type', 'unknown')}"
                            )

                            # ë„êµ¬ ì‹¤í–‰ ê³¼ì •ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ë‹¬
                            if chunk.get("type") in ["progress", "result", "error"]:
                                yield {
                                    "type": "tool_execution",
                                    "stage": chunk.get("stage", "unknown"),
                                    "message": chunk.get("message", ""),
                                    "progress": step_progress,
                                    "step_id": step_id,
                                    "chunk_data": chunk,
                                }

                            # ìµœì¢… ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ì €ì¥
                            if chunk.get("type") == "result":
                                final_result = {
                                    "step_id": step_id,
                                    "status": "success",
                                    "result": chunk.get("data", {}),
                                    "final_result": str(chunk.get("data", {}))[:500],
                                }
                                print(
                                    f"         âœ… [RESULT] ìµœì¢… ê²°ê³¼ ì €ì¥: {final_result['status']}"
                                )

                        if final_result:
                            execution_results.append(final_result)
                            print(
                                f"      âœ… [STEP 3.{i+1}] ì™„ë£Œ - ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ì €ì¥ë¨"
                            )
                        else:
                            error_result = {
                                "step_id": step_id,
                                "status": "error",
                                "error": "ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ê²°ê³¼ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤",
                            }
                            execution_results.append(error_result)
                            print(f"      âŒ [STEP 3.{i+1}] ì‹¤íŒ¨ - ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ì—†ìŒ")
                    else:
                        # ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                        print(f"      ğŸ”„ [EXECUTION] ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì‹œë„...")
                        result = self._execute_single_step(step, execution_context)
                        execution_results.append(result)
                        print(
                            f"      âœ… [STEP 3.{i+1}] ì™„ë£Œ - ë¹„ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼: {result.get('status', 'unknown')}"
                        )

                except Exception as e:
                    error_result = {
                        "step_id": step_id,
                        "status": "error",
                        "error": str(e),
                    }
                    execution_results.append(error_result)
                    print(f"      âŒ [STEP 3.{i+1}] ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

            print(
                f"\n   âœ… [STEP 3] í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ì™„ë£Œ: {len(execution_results)}ê°œ ê²°ê³¼"
            )
            for i, result in enumerate(execution_results):
                print(
                    f"      ê²°ê³¼ {i+1}: {result.get('step_id', 'unknown')} - {result.get('status', 'unknown')}"
                )

            yield {
                "type": "progress",
                "stage": "trace_analysis",
                "message": "ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "progress": 0.8,
            }

            # 4ë‹¨ê³„: Trace Manager - ì „ì²´ ì¶”ë¡  ê³¼ì • ë¶„ì„
            print(f"\nğŸ“Š [STEP 4] Trace Manager ì‹¤í–‰ ì¤‘...")
            trace_analysis = self._analyze_execution_trace(
                execution_results, execution_context
            )
            print(
                f"   âœ… [TRACE] ë¶„ì„ ì™„ë£Œ: {trace_analysis.get('final_assessment', {}).get('workflow_status', 'unknown')}"
            )

            yield {
                "type": "progress",
                "stage": "final_response",
                "message": "ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "progress": 0.9,
            }

            # 5ë‹¨ê³„: Answer Agent - ìµœì¢… ì‘ë‹µ ìƒì„±
            print(f"\nâœ¨ [STEP 5] Answer Agent ì‹¤í–‰ ì¤‘...")
            final_response = self._generate_final_response(
                execution_results, trace_analysis, execution_context
            )
            print(f"   âœ… [ANSWER] ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ")

            total_time = time.time() - start_time

            # ìµœì¢… ê²°ê³¼
            final_data = {
                **final_response,
                "hybrid_execution_summary": {
                    "total_execution_time": f"{total_time:.2f}ì´ˆ",
                    "steps_executed": len(execution_results),
                    "successful_steps": len(
                        [r for r in execution_results if r.get("status") == "success"]
                    ),
                    "intent": intent,
                },
                "streaming": True,
            }

            print(f"\nğŸ‰ [ORCHESTRATOR] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
            print(
                f"   ğŸ“Š ì„±ê³µí•œ ë‹¨ê³„: {final_data['hybrid_execution_summary']['successful_steps']}/{final_data['hybrid_execution_summary']['steps_executed']}"
            )

            yield {
                "type": "result",
                "stage": "completed",
                "message": "ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "progress": 1.0,
                "data": final_data,
            }

        except Exception as e:
            print(f"\nâŒ [ORCHESTRATOR] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback

            traceback.print_exc()

            yield {
                "type": "error",
                "stage": "streaming_error",
                "message": f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "error": str(e),
                "progress": 0.0,
            }

    def _execute_step_streaming(
        self, step: Dict[str, Any], context: Dict[str, Any]
    ) -> Generator:
        """
        ê°œë³„ ë‹¨ê³„ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤í–‰

        Args:
            step: ì‹¤í–‰í•  ë‹¨ê³„
            context: ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸

        Returns:
            Generator ë˜ëŠ” None (ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
        """
        step_type = step.get("step_type", "general")
        required_tools = step.get("required_tools", [])
        step_id = step.get("step_id", "unknown")

        print(f"         ğŸ¯ [STREAMING] ë‹¨ê³„ íƒ€ì…: {step_type}, ë„êµ¬: {required_tools}")

        # ìŠ¬ë¼ì´ë“œ ìƒì„± ë‹¨ê³„ì¸ ê²½ìš° ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
        if any(
            tool in required_tools
            for tool in ["slide_formatter", "format_slide", "slide_generator"]
        ):
            print(f"         ğŸ¨ [STREAMING] ìŠ¬ë¼ì´ë“œ ìƒì„± ë‹¨ê³„ ê°ì§€")

            try:
                # LangChain SlideFormatterì˜ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ ì§ì ‘ í™œìš©
                content = context.get("user_input", "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê°œìš”")

                # ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì½˜í…ì¸  ì •ì œ
                if "ìŠ¬ë¼ì´ë“œ" in content or "slide" in content.lower():
                    content = (
                        content.replace("ìŠ¬ë¼ì´ë“œ", "").replace("slide", "").strip()
                    )
                    content = content or "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê°œìš”"

                slide_inputs = {
                    "content": content,
                    "title": "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤",
                    "slide_type": "basic",
                    "subtitle": "",
                    "format": "json",
                }

                print(f"         ğŸ“‹ [STREAMING] ìŠ¬ë¼ì´ë“œ ì…ë ¥: {slide_inputs}")

                # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                print(f"         â–¶ï¸  [STREAMING] SlideFormatter ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
                chunk_count = 0
                for chunk in self.slide_formatter.run_streaming(slide_inputs):
                    chunk_count += 1
                    print(
                        f"            ğŸ“¦ [SLIDE CHUNK {chunk_count}] {chunk.get('type', 'unknown')}: {chunk.get('message', '')}"
                    )
                    yield chunk

                print(
                    f"         âœ… [STREAMING] SlideFormatter ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ({chunk_count}ê°œ ì²­í¬)"
                )
                return

            except Exception as e:
                print(f"         âŒ [STREAMING] ìŠ¬ë¼ì´ë“œ ìƒì„± ì˜¤ë¥˜: {str(e)}")
                yield {
                    "type": "error",
                    "stage": "slide_generation_error",
                    "message": f"ìŠ¬ë¼ì´ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}",
                    "progress": 0.0,
                    "error": str(e),
                }
                return

        # MCP ë„êµ¬ ì‹¤í–‰ì´ í•„ìš”í•œ ê²½ìš°
        elif any(
            tool in required_tools
            for tool in [
                "rag_retriever",
                "search_documents",
                "data_analyzer",
                "content_validator",
            ]
        ):
            print(f"         ğŸ” [STREAMING] MCP ë„êµ¬ ì‹¤í–‰ í•„ìš” ê°ì§€")

            try:
                # MCP ë„êµ¬ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ë°˜í™˜
                print(f"         ğŸ”§ [MCP] ë¹„ë™ê¸° MCP ë„êµ¬ ì‹¤í–‰ ì‹œì‘...")

                # ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë°
                yield {
                    "type": "progress",
                    "stage": "mcp_tool_execution",
                    "message": "MCP ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                    "progress": 0.3,
                }

                # ì‹¤ì œ MCP ë„êµ¬ ì‹¤í–‰
                result = self._execute_single_step(step, context)

                print(
                    f"         âœ… [MCP] ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ: {result.get('status', 'unknown')}"
                )

                # ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ë°˜í™˜
                yield {
                    "type": "result",
                    "stage": "mcp_completed",
                    "message": "MCP ë„êµ¬ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "progress": 1.0,
                    "data": result,
                }
                return

            except Exception as e:
                print(f"         âŒ [MCP] ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
                yield {
                    "type": "error",
                    "stage": "mcp_execution_error",
                    "message": f"MCP ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                    "progress": 0.0,
                    "error": str(e),
                }
                return

        # ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° None ë°˜í™˜
        print(f"         â­ï¸  [STREAMING] ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì› ë‹¨ê³„")
        return None

    def _execute_single_step(
        self, step: Dict[str, Any], context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ê°œë³„ ë‹¨ê³„ ì‹¤í–‰ (MCP ë„êµ¬ ì§ì ‘ ì‹¤í–‰ ë˜ëŠ” ReAct ì‹¤í–‰ê¸° ì‚¬ìš©)

        Args:
            step: ì‹¤í–‰í•  ë‹¨ê³„
            context: ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸

        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        step_id = step.get("step_id", "unknown")
        step_type = step.get("step_type", "general")
        required_tools = step.get("required_tools", [])
        step_description = step.get("description", "")

        print(f"      ğŸ”„ [SINGLE_STEP] ë‹¨ê³„ ì‹¤í–‰ ì‹œì‘: {step_id}")
        print(f"         ğŸ“ ì„¤ëª…: {step_description}")
        print(f"         ğŸ› ï¸  ë„êµ¬: {required_tools}")
        print(f"         ğŸ“Š íƒ€ì…: {step_type}")

        try:
            # ë„êµ¬ ì´ë¦„ ì •ê·œí™”
            print(f"         ğŸ”§ [NORMALIZE] ë„êµ¬ ì´ë¦„ ì •ê·œí™” ì‹œì‘...")
            normalized_tools = []
            for tool in required_tools:
                if tool in [
                    "rag_retriever",
                    "search_documents",
                    "data_analyzer",
                    "content_validator",
                ]:
                    normalized_tools.append("search_documents")
                    print(f"            âœ… '{tool}' â†’ 'search_documents'")
                elif tool in ["slide_formatter", "format_slide", "slide_generator"]:
                    # ìŠ¬ë¼ì´ë“œ í¬ë§·íŒ…ì€ LangChain Toolë¡œ ì§ì ‘ ì²˜ë¦¬
                    normalized_tools.append("slide_formatter_langchain")
                    print(f"            âœ… '{tool}' â†’ 'slide_formatter_langchain'")
                elif tool in [
                    "report_summary",
                    "summarize_report",
                    "content_generator",
                ]:
                    normalized_tools.append("summarize_report")
                    print(f"            âœ… '{tool}' â†’ 'summarize_report'")
                elif tool in ["get_tool_status"]:
                    normalized_tools.append("get_tool_status")
                    print(f"            âœ… '{tool}' â†’ 'get_tool_status'")
                else:
                    normalized_tools.append("search_documents")
                    print(f"            âš ï¸ '{tool}' â†’ 'search_documents' (ê¸°ë³¸ê°’)")

            print(f"         ğŸ“‹ [NORMALIZE] ì •ê·œí™”ëœ ë„êµ¬: {normalized_tools}")

            # LangChain Tool ì§ì ‘ ì‹¤í–‰ (ìŠ¬ë¼ì´ë“œ í¬ë§·íŒ…)
            if "slide_formatter_langchain" in normalized_tools:
                print(f"         ğŸ¨ [LANGCHAIN] SlideFormatter ë„êµ¬ ì§ì ‘ ì‹¤í–‰")

                # ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
                user_input = context.get("user_input", "")
                if "ìŠ¬ë¼ì´ë“œ" in user_input or "slide" in user_input.lower():
                    content = (
                        user_input.replace("ìŠ¬ë¼ì´ë“œ", "").replace("slide", "").strip()
                    )
                    content = content or "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê°œìš”"
                else:
                    content = "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê°œìš”"

                slide_inputs = {
                    "content": content,
                    "title": "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤",
                    "slide_type": "basic",
                    "subtitle": "",
                    "format": "json",
                }

                print(f"            ğŸ“‹ [LANGCHAIN] ìŠ¬ë¼ì´ë“œ ì…ë ¥: {slide_inputs}")
                print(f"            â–¶ï¸  [LANGCHAIN] SlideFormatter ì‹¤í–‰ ì¤‘...")

                result = self.slide_formatter.run(slide_inputs)

                print(f"            âœ… [LANGCHAIN] SlideFormatter ì‹¤í–‰ ì™„ë£Œ")
                print(f"            ğŸ“Š [LANGCHAIN] ê²°ê³¼ íƒ€ì…: {type(result)}")

                return {
                    "step_id": step_id,
                    "step_type": step_type,
                    "tool": "slide_formatter_langchain",
                    "status": "success",
                    "result": result,
                    "final_result": str(result.get("html", ""))[:500],
                }

            # MCP ë„êµ¬ ì‹¤í–‰ (ë‹¨ì¼ ë„êµ¬)
            elif len(normalized_tools) == 1 and normalized_tools[0] in [
                "search_documents",
                "summarize_report",
                "get_tool_status",
            ]:
                tool_name = normalized_tools[0]
                print(f"         ğŸ”§ [MCP] MCP ë„êµ¬ ì‹¤í–‰: {tool_name}")

                # MCP ë„êµ¬ ì‹¤í–‰ì„ ìœ„í•œ ë¹„ë™ê¸° í•¨ìˆ˜
                async def execute_mcp_tool():
                    try:
                        print(f"            ğŸ”— [MCP] MCP í´ë¼ì´ì–¸íŠ¸ í™•ì¸...")
                        if not self.mcp_multi_client:
                            raise Exception("MCP í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

                        print(f"            ğŸ“‹ [MCP] MCP ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
                        # MCP ë„êµ¬ë“¤ ê°€ì ¸ì˜¤ê¸°
                        tools = await self._get_mcp_tools()
                        print(f"            ğŸ“Š [MCP] ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ìˆ˜: {len(tools)}")

                        if tools:
                            tool_names = [tool.name for tool in tools]
                            print(f"            ğŸ“‹ [MCP] ë„êµ¬ ëª©ë¡: {tool_names}")

                        # í•´ë‹¹ ë„êµ¬ ì°¾ê¸°
                        target_tool = None
                        for tool in tools:
                            if tool.name == tool_name:
                                target_tool = tool
                                break

                        if not target_tool:
                            available_tools = (
                                [tool.name for tool in tools] if tools else []
                            )
                            raise Exception(
                                f"MCP ë„êµ¬ '{tool_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {available_tools}"
                            )

                        print(
                            f"            âœ… [MCP] ëŒ€ìƒ ë„êµ¬ ë°œê²¬: {target_tool.name}"
                        )

                        # ë„êµ¬ë³„ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
                        if tool_name == "search_documents":
                            params = step.get("parameters", {})
                            if not params:
                                description = step.get("description", "")
                                user_input = context.get("user_input", "")

                                # ê²€ìƒ‰ ì¿¼ë¦¬ ê²°ì •
                                if (
                                    "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤" in description
                                    or "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤" in user_input
                                ):
                                    query = "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤"
                                elif "ë³´ì•ˆ" in description or "ë³´ì•ˆ" in user_input:
                                    query = "í´ë¼ìš°ë“œ ë³´ì•ˆ"
                                elif "ì •ì±…" in description or "ì •ì±…" in user_input:
                                    query = "í´ë¼ìš°ë“œ ì •ì±…"
                                else:
                                    query = user_input[:50] or "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤"

                                params = {"query": query, "top_k": 5}

                            print(
                                f"            ğŸ“‹ [MCP] search_documents ë§¤ê°œë³€ìˆ˜: {params}"
                            )
                            print(f"            â–¶ï¸  [MCP] search_documents ì‹¤í–‰ ì¤‘...")
                            result = await target_tool.ainvoke(params)
                            print(f"            âœ… [MCP] search_documents ì‹¤í–‰ ì™„ë£Œ")

                        elif tool_name == "summarize_report":
                            params = step.get("parameters", {})
                            if not params:
                                params = {
                                    "content": context.get(
                                        "user_input", "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ë³´ê³ ì„œ"
                                    ),
                                    "title": "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ë³´ê³ ì„œ",
                                    "summary_type": "executive",
                                    "format_type": "html",
                                }

                            print(
                                f"            ğŸ“‹ [MCP] summarize_report ë§¤ê°œë³€ìˆ˜: {params}"
                            )
                            print(f"            â–¶ï¸  [MCP] summarize_report ì‹¤í–‰ ì¤‘...")
                            result = await target_tool.ainvoke(params)
                            print(f"            âœ… [MCP] summarize_report ì‹¤í–‰ ì™„ë£Œ")

                        elif tool_name == "get_tool_status":
                            print(f"            â–¶ï¸  [MCP] get_tool_status ì‹¤í–‰ ì¤‘...")
                            result = await target_tool.ainvoke({})
                            print(f"            âœ… [MCP] get_tool_status ì‹¤í–‰ ì™„ë£Œ")

                        print(f"            ğŸ“Š [MCP] ê²°ê³¼ íƒ€ì…: {type(result)}")
                        print(
                            f"            ğŸ“‹ [MCP] ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {str(result)[:200]}..."
                        )

                        return {
                            "step_id": step_id,
                            "step_type": step_type,
                            "tool": tool_name,
                            "status": "success",
                            "result": result,
                            "final_result": str(result)[:500],
                        }

                    except Exception as e:
                        print(f"            âŒ [MCP] ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
                        import traceback

                        traceback.print_exc()
                        return {
                            "step_id": step_id,
                            "step_type": step_type,
                            "tool": tool_name,
                            "status": "error",
                            "error": str(e),
                        }

                # ë¹„ë™ê¸° MCP ë„êµ¬ ì‹¤í–‰
                print(f"            ğŸ”„ [MCP] ë¹„ë™ê¸° ì‹¤í–‰ ì‹œì‘...")
                result = self._run_async_mcp_operation(execute_mcp_tool())
                print(
                    f"            âœ… [MCP] ë¹„ë™ê¸° ì‹¤í–‰ ì™„ë£Œ: {result.get('status', 'unknown')}"
                )
                return result

            else:
                # ReAct ì‹¤í–‰ê¸°ë¥¼ í†µí•œ ì‹¤í–‰ (ë³µí•© ë„êµ¬ ë˜ëŠ” ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš°)
                print(f"         ğŸ¤– [REACT] ReAct Executorë¡œ ì „ë‹¬: {normalized_tools}")
                executor = self._get_or_create_executor(step_id)
                print(f"            ğŸ“‹ [REACT] Executor ID: {step_id}")
                print(f"            â–¶ï¸  [REACT] ì‹¤í–‰ ì¤‘...")
                result = executor.execute_step(step, context)
                print(
                    f"            âœ… [REACT] ì‹¤í–‰ ì™„ë£Œ: {result.get('status', 'unknown')}"
                )
                return result

        except Exception as e:
            print(f"         âŒ [SINGLE_STEP] ë‹¨ê³„ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            import traceback

            traceback.print_exc()
            return {
                "step_id": step_id,
                "step_type": step_type,
                "tool": required_tools[0] if required_tools else "unknown",
                "status": "error",
                "error": str(e),
            }

    def _process_hybrid_execution(
        self,
        plan_result: Dict[str, Any],
        router_result: Dict[str, Any],
        user_input: str,
        start_time: float,
    ) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ë°©ì‹ ì²˜ë¦¬"""
        execution_steps = plan_result.get("execution_steps", [])
        dependency_graph = plan_result.get("dependency_graph", {})

        print(f"   â”” ì´ ì‹¤í–‰ ë‹¨ê³„: {len(execution_steps)}ê°œ")
        print(
            f"   â”” ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥: {len(dependency_graph.get('parallel_groups', []))}ê°œ ê·¸ë£¹"
        )

        # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (Plan & Execute + ReAct)
        print("\nğŸ”„ 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (Plan & Execute + ReAct)")
        execution_context = {
            "user_input": user_input,
            "intent": router_result.get("intent"),
            "key_entities": router_result.get("key_entities", []),
            "execution_steps": execution_steps,  # í‚¤ ì´ë¦„ ìˆ˜ì •!
            "execution_plan": execution_steps,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
            "dependency_graph": dependency_graph,
        }

        # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (ë‹¨ê³„ë³„ ì‹¤í–‰)
        print("\nâš¡ 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰")
        execution_results = []

        for step in execution_steps:
            try:
                # ë‹¨ê³„ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì› í™•ì¸)
                step_result = self._execute_step_streaming(step, execution_context)
                if step_result:
                    # ìŠ¤íŠ¸ë¦¬ë° ì§€ì›í•˜ëŠ” ê²½ìš° - ë§ˆì§€ë§‰ ê²°ê³¼ë§Œ ìˆ˜ì§‘
                    final_result = None
                    for chunk in step_result:
                        if chunk.get("type") == "result":
                            final_result = {
                                "step_id": step.get("step_id"),
                                "status": "success",
                                "result": chunk.get("data", {}),
                                "final_result": str(chunk.get("data", {}))[:500],
                            }

                    if final_result:
                        execution_results.append(final_result)
                    else:
                        # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ
                        execution_results.append(
                            {
                                "step_id": step.get("step_id"),
                                "status": "error",
                                "error": "ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ê²°ê³¼ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤",
                            }
                        )
                else:
                    # ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                    result = self._execute_single_step(step, execution_context)
                    execution_results.append(result)

            except Exception as e:
                execution_results.append(
                    {
                        "step_id": step.get("step_id"),
                        "status": "error",
                        "error": str(e),
                    }
                )

        print(f"   âœ… ì‹¤í–‰ ì™„ë£Œ: {len(execution_results)}ê°œ ë‹¨ê³„")

        # 4ë‹¨ê³„: Trace Manager - ì „ì²´ ì¶”ë¡  ê³¼ì • ë¶„ì„
        print("\nğŸ“Š 4ë‹¨ê³„: Trace Manager - ì¶”ë¡  ê³¼ì • ë¶„ì„")
        trace_analysis = self._analyze_execution_trace(
            execution_results, execution_context
        )

        # 5ë‹¨ê³„: ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬ (í•„ìš”ì‹œ)
        if trace_analysis.get("final_assessment", {}).get("next_action") in [
            "retry",
            "revise",
        ]:
            print("\nğŸ”§ 5ë‹¨ê³„: ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬")
            recovery_result = self._handle_failure_recovery(
                execution_results, execution_context, trace_analysis
            )
            if recovery_result.get("recovery_status") == "success":
                execution_results = recovery_result.get(
                    "recovered_results", execution_results
                )

        # 6ë‹¨ê³„: Answer Agent - ìµœì¢… ì‘ë‹µ ìƒì„±
        print("\nâœ¨ 6ë‹¨ê³„: Answer Agent - ìµœì¢… ì‘ë‹µ ìƒì„±")
        final_response = self._generate_final_response(
            execution_results, trace_analysis, execution_context
        )

        # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total_time = time.time() - start_time

        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        final_result = {
            **final_response,
            "hybrid_execution_summary": {
                "total_execution_time": f"{total_time:.2f}ì´ˆ",
                "steps_executed": len(execution_results),
                "successful_steps": len(
                    [r for r in execution_results if r.get("status") == "success"]
                ),
                "reasoning_depth": trace_analysis.get("performance_metrics", {}).get(
                    "reasoning_depth", "medium"
                ),
                "overall_confidence": trace_analysis.get("final_assessment", {}).get(
                    "confidence", 0.5
                ),
            },
            "mcp_context": {
                **self.mcp_context,
                "processing_flow": [
                    f"Router: {router_result.get('intent', 'unknown')}",
                    f"Planner: {len(execution_steps)} steps planned",
                    f"Execution: {len(execution_results)} steps executed",
                    f"Trace Analysis: {trace_analysis.get('final_assessment', {}).get('workflow_status', 'unknown')}",
                    "Answer: completed",
                ],
                "status": "success",
                "hybrid_mode_used": True,
                "total_time": total_time,
            },
        }

        print(f"\nâœ… í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
        return final_result

    def _generate_direct_answer(self, user_input: str) -> str:
        """
        ì¼ë°˜ì ì¸ ëŒ€í™”ë¥¼ ìœ„í•œ ì§ì ‘ ì‘ë‹µ ìƒì„±

        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥

        Returns:
            str: ì§ì ‘ ì‘ë‹µ
        """
        # ê°„ë‹¨í•œ ì¸ì‚¬ë‚˜ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
        user_input_lower = user_input.lower()

        if any(
            greeting in user_input_lower
            for greeting in ["ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "ì‹œì‘"]
        ):
            return """
ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ 

ì €ëŠ” í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ê²ƒë“¤:**
â€¢ í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€
â€¢ ì •ì±… ë° ì»´í”Œë¼ì´ì–¸ìŠ¤ ê°€ì´ë“œ
â€¢ ìŠ¬ë¼ì´ë“œ ë° í”„ë ˆì  í…Œì´ì…˜ ìë£Œ ìƒì„±
â€¢ ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬ ë°©ì•ˆ ì œì‹œ

ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?
"""

        elif any(
            help_word in user_input_lower
            for help_word in ["ë„ì›€", "help", "ë­ í•  ìˆ˜", "ê¸°ëŠ¥"]
        ):
            return """
**í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ AI ì–´ì‹œìŠ¤í„´íŠ¸ ê¸°ëŠ¥ ì•ˆë‚´** ğŸ“š

ğŸ” **ì§ˆë¬¸ ì‘ë‹µ**
- í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ì •ì±…
- ì»´í”Œë¼ì´ì–¸ìŠ¤ ìš”êµ¬ì‚¬í•­
- ë³´ì•ˆ ê´€ë¦¬ ë°©ì•ˆ
- ëª¨ë‹ˆí„°ë§ ì „ëµ

ğŸ“Š **ìŠ¬ë¼ì´ë“œ ìƒì„±**
- í”„ë ˆì  í…Œì´ì…˜ ìë£Œ ì‘ì„±
- ê°œë… ì •ë¦¬ ìŠ¬ë¼ì´ë“œ
- ë¹„êµ ë¶„ì„ ìë£Œ

ì˜ˆì‹œ: "í´ë¼ìš°ë“œ ë³´ì•ˆ ì •ì±…ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”" ë˜ëŠ” "ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ìŠ¬ë¼ì´ë“œ ë§Œë“¤ì–´ì£¼ì„¸ìš”"
"""

        else:
            return """
í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ì™€ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ í•´ì£¼ì‹œë©´ ë” ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´:
â€¢ "í´ë¼ìš°ë“œ ë³´ì•ˆ ì •ì±…ì´ ë¬´ì—‡ì¸ê°€ìš”?"
â€¢ "ì»´í”Œë¼ì´ì–¸ìŠ¤ ê´€ë¦¬ ë°©ì•ˆ ìŠ¬ë¼ì´ë“œ ë§Œë“¤ì–´ì£¼ì„¸ìš”"
â€¢ "ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"

ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ˜Š
"""

    def _get_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime

        return datetime.now().isoformat()

    def _get_or_create_executor(self, executor_id: str) -> ReActExecutorAgent:
        """ReAct Executor ìƒì„± ë˜ëŠ” ê¸°ì¡´ ê²ƒ ë°˜í™˜"""
        if executor_id not in self.executor_pool:
            if len(self.executor_pool) >= self.max_executors:
                # í’€ì´ ê°€ë“ ì°¬ ê²½ìš° ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°
                oldest_key = next(iter(self.executor_pool))
                del self.executor_pool[oldest_key]

            self.executor_pool[executor_id] = ReActExecutorAgent(executor_id)

        return self.executor_pool[executor_id]

    def _analyze_execution_trace(
        self, execution_results: List[Dict[str, Any]], context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì „ì²´ ì‹¤í–‰ ì¶”ì  ë¶„ì„"""
        trace_input = {
            "execution_results": execution_results,
            "failed_steps": [
                r
                for r in execution_results
                if r.get("status") not in ["success", "partial_success"]
            ],
            "context": context,
        }

        return self.trace_manager(trace_input)

    def _handle_failure_recovery(
        self,
        execution_results: List[Dict[str, Any]],
        context: Dict[str, Any],
        trace_analysis: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬"""
        failed_steps = [
            r
            for r in execution_results
            if r.get("status") not in ["success", "partial_success"]
        ]

        if not failed_steps:
            return {"recovery_status": "no_recovery_needed"}

        # ê¸°ë³¸ ë³µêµ¬: ë‹¨ìˆœíˆ ì¬ì‹œë„ ê¶Œì¥
        return {
            "recovery_status": "completed",
            "recovery_strategy": "retry_recommended",
            "recovered_results": execution_results,
        }

    def _generate_final_response(
        self,
        execution_results: List[Dict[str, Any]],
        trace_analysis: Dict[str, Any],
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ìµœì¢… ì‘ë‹µ ìƒì„±"""
        # ì„±ê³µí•œ ê²°ê³¼ë“¤ì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ
        successful_results = [
            r for r in execution_results if r.get("status") == "success"
        ]

        if successful_results:
            latest_result = successful_results[-1]
            answer_content = latest_result.get("final_result", "")
        else:
            # ë¶€ë¶„ ì„±ê³µì´ë¼ë„ ì‚¬ìš©
            partial_results = [
                r for r in execution_results if r.get("status") == "partial_success"
            ]
            if partial_results:
                answer_content = partial_results[-1].get("final_result", "")
            else:
                answer_content = "ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        # Answer Agent ì…ë ¥ êµ¬ì„±
        answer_input = {
            "agent_type": "hybrid_execution",
            "answer_content": answer_content,
            "execution_results": execution_results,
            "reasoning_trace": self.reasoning_trace_logger.get_global_trace(),
            "trace_summary": trace_analysis.get("trace_summary", {}),
            "overall_confidence": trace_analysis.get("final_assessment", {}).get(
                "confidence", 0.5
            ),
            "source_type": "hybrid_react",
        }

        return self.answer_agent(answer_input)

    def _create_error_response(
        self, error_message: str, execution_time: float
    ) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±"""
        return {
            "final_answer": f"ì‹œìŠ¤í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message}\n\ní•˜ì´ë¸Œë¦¬ë“œ AI ì‹œìŠ¤í…œì´ ë³µêµ¬ë¥¼ ì‹œë„í–ˆì§€ë§Œ ì™„ì „í•œ ì²˜ë¦¬ê°€ ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
            "timestamp": self._get_timestamp(),
            "hybrid_execution_summary": {
                "total_execution_time": f"{execution_time:.2f}ì´ˆ",
                "steps_executed": 0,
                "successful_steps": 0,
                "reasoning_depth": "error",
                "overall_confidence": 0.0,
            },
            "mcp_context": {
                **self.mcp_context,
                "status": "error",
                "error_message": error_message,
                "hybrid_mode_used": False,
                "total_time": execution_time,
            },
        }

    def get_system_status(self) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸

        Returns:
            Dict[str, Any]: ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
        """
        # MCP ë„êµ¬ ìƒíƒœ í™•ì¸
        mcp_tools_status = "unavailable"
        try:
            if self.mcp_multi_client:
                # ë¹„ë™ê¸° ë„êµ¬ ìƒíƒœ í™•ì¸
                async def check_mcp_status():
                    try:
                        tools = await self._get_mcp_tools()
                        return "available" if len(tools) > 0 else "empty"
                    except:
                        return "error"

                mcp_tools_status = self._run_async_mcp_operation(check_mcp_status())
            else:
                mcp_tools_status = "not_initialized"
        except Exception as e:
            mcp_tools_status = f"error: {str(e)}"

        return {
            "orchestrator": "hybrid_running",
            "agents": {
                "router": "initialized",
                "enhanced_planner": "initialized",
                "answer": "enhanced",
                "trace_manager": "initialized",
            },
            "react_executors": {
                "pool_size": len(self.executor_pool),
                "max_executors": self.max_executors,
                "active_executors": list(self.executor_pool.keys()),
            },
            "tools": {
                "reasoning_trace_logger": "active",
                "plan_revision_tool": "active",
                "state_manager": "active",
                "slide_formatter_langchain": "available",
                "mcp_tools": mcp_tools_status,
            },
            "mcp_integration": {
                "multi_client_initialized": self.mcp_multi_client is not None,
                "legacy_client_available": self.mcp_client is not None,
                "tools_status": mcp_tools_status,
            },
            "hybrid_features": {
                "parallel_execution": False,  # í–¥í›„ êµ¬í˜„
                "react_reasoning": True,
                "failure_recovery": True,
                "trace_analysis": True,
                "streaming_support": True,
            },
            "mcp_context": self.mcp_context,
        }

    def clear_execution_state(self):
        """ì‹¤í–‰ ìƒíƒœ ì´ˆê¸°í™”"""
        self.executor_pool.clear()
        self.reasoning_trace_logger.clear_traces()
        self.plan_revision_tool.clear_history()
        self.state_manager.clear_all_states()
        print("ğŸ§¹ í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")
