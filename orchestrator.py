from typing import Dict, Any, List, Generator
import time

from agents import (
    RouterAgent,
    PlannerAgent,
    AnswerAgent,
    ReActExecutorAgent,
    TraceManagerAgent,
)
from tools import ReasoningTraceLogger, PlanRevisionTool, StateManager
from mcp_client import get_mcp_client


class CloudGovernanceOrchestrator:
    """
    í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ AI ì‹œìŠ¤í…œ í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
    Plan & Execute + ReAct í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬
    """

    def __init__(self):
        self.router_agent = RouterAgent()
        self.planner_agent = PlannerAgent()
        self.answer_agent = AnswerAgent()

        self.mcp_client = get_mcp_client()

        # ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì„± ìš”ì†Œë“¤
        self.trace_manager = TraceManagerAgent()
        self.reasoning_trace_logger = ReasoningTraceLogger()
        self.plan_revision_tool = PlanRevisionTool()
        self.state_manager = StateManager()

        # ReAct Executor Pool
        self.executor_pool = {}
        self.max_executors = 5

        self.mcp_context = {
            "role": "hybrid_orchestrator",
            "function": "hybrid_workflow_coordination",
            "agents_initialized": True,
            "hybrid_mode": True,
            "mcp_tools_available": True,
        }

    def process_request(self, user_input: str) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ

        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥

        Returns:
            Dict[str, Any]: ìµœì¢… ì‘ë‹µ
        """
        start_time = time.time()

        try:
            print(f"ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì‹œì‘: {user_input[:50]}...")

            # 1ë‹¨ê³„: Router Agent - ì˜ë„ ë¶„ì„
            print("\nğŸ“ 1ë‹¨ê³„: Router Agent - ì˜ë„ ë¶„ì„")
            router_result = self.router_agent({"user_input": user_input})
            print(f"   â”” Intent: {router_result.get('intent', 'unknown')}")

            # 2ë‹¨ê³„: Enhanced Planner Agent - í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
            print("\nğŸ“‹ 2ë‹¨ê³„: Enhanced Planner - í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½")
            planner_input = {**router_result, "user_input": user_input}
            plan_result = self.planner_agent(planner_input)

            return self._process_hybrid_execution(
                plan_result, router_result, user_input, start_time
            )

        except Exception as e:
            error_time = time.time() - start_time
            print(f"\nâŒ í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì˜¤ë¥˜: {str(e)}")
            return self._create_error_response(str(e), error_time)

    def process_request_streaming(
        self, user_input: str
    ) -> Generator[Dict[str, Any], None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ

        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥

        Yields:
            Dict[str, Any]: ìŠ¤íŠ¸ë¦¬ë° ì²­í¬
        """
        start_time = time.time()

        try:
            yield {
                "type": "progress",
                "stage": "router_analysis",
                "message": "ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "progress": 0.1,
            }

            # 1ë‹¨ê³„: Router Agent - ì˜ë„ ë¶„ì„
            router_result = self.router_agent({"user_input": user_input})
            intent = router_result.get("intent", "unknown")

            yield {
                "type": "progress",
                "stage": "planner_analysis",
                "message": f"ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ìˆìŠµë‹ˆë‹¤... (ì˜ë„: {intent})",
                "progress": 0.2,
                "intent": intent,
            }

            # 2ë‹¨ê³„: Enhanced Planner Agent - í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
            planner_input = {**router_result, "user_input": user_input}
            plan_result = self.planner_agent(planner_input)

            execution_steps = plan_result.get("execution_steps", [])
            dependency_graph = plan_result.get("dependency_graph", {})

            yield {
                "type": "progress",
                "stage": "execution_start",
                "message": f"{len(execution_steps)}ê°œ ë‹¨ê³„ì˜ ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...",
                "progress": 0.3,
                "steps_count": len(execution_steps),
            }

            # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)
            execution_context = {
                "user_input": user_input,
                "intent": router_result.get("intent"),
                "key_entities": router_result.get("key_entities", []),
                "execution_steps": execution_steps,
                "execution_plan": execution_steps,
                "dependency_graph": dependency_graph,
            }

            # ë‹¨ê³„ë³„ ì‹¤í–‰ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬
            execution_results = []
            for i, step in enumerate(execution_steps):
                step_progress = 0.3 + (0.5 * (i + 1) / len(execution_steps))

                yield {
                    "type": "progress",
                    "stage": "step_execution",
                    "message": f"ë‹¨ê³„ {i+1}/{len(execution_steps)} ì‹¤í–‰ ì¤‘: {step.get('description', 'Unknown step')}",
                    "progress": step_progress,
                    "current_step": step.get("step_id", f"step_{i+1}"),
                }

                try:
                    # ë‹¨ê³„ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
                    step_result = self._execute_step_streaming(step, execution_context)
                    if step_result:
                        for chunk in step_result:
                            # ë„êµ¬ ì‹¤í–‰ ê³¼ì •ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ë‹¬
                            if chunk.get("type") in ["progress", "result", "error"]:
                                yield {
                                    "type": "tool_execution",
                                    "stage": chunk.get("stage", "unknown"),
                                    "message": chunk.get("message", ""),
                                    "progress": step_progress,
                                    "step_id": step.get("step_id"),
                                    "chunk_data": chunk,
                                }

                            # ìµœì¢… ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ì €ì¥
                            if chunk.get("type") == "result":
                                execution_results.append(
                                    {
                                        "step_id": step.get("step_id"),
                                        "status": "success",
                                        "result": chunk.get("data", {}),
                                        "final_result": str(chunk.get("data", {}))[
                                            :500
                                        ],
                                    }
                                )
                    else:
                        # ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                        result = self._execute_single_step(step, execution_context)
                        execution_results.append(result)

                except Exception as e:
                    execution_results.append(
                        {
                            "step_id": step.get("step_id"),
                            "status": "error",
                            "error": str(e),
                        }
                    )

            yield {
                "type": "progress",
                "stage": "trace_analysis",
                "message": "ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "progress": 0.8,
            }

            # 4ë‹¨ê³„: Trace Manager - ì „ì²´ ì¶”ë¡  ê³¼ì • ë¶„ì„
            trace_analysis = self._analyze_execution_trace(
                execution_results, execution_context
            )

            yield {
                "type": "progress",
                "stage": "final_response",
                "message": "ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "progress": 0.9,
            }

            # 5ë‹¨ê³„: Answer Agent - ìµœì¢… ì‘ë‹µ ìƒì„±
            final_response = self._generate_final_response(
                execution_results, trace_analysis, execution_context
            )

            total_time = time.time() - start_time

            # ìµœì¢… ê²°ê³¼
            yield {
                "type": "result",
                "stage": "completed",
                "message": "ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "progress": 1.0,
                "data": {
                    **final_response,
                    "hybrid_execution_summary": {
                        "total_execution_time": f"{total_time:.2f}ì´ˆ",
                        "steps_executed": len(execution_results),
                        "successful_steps": len(
                            [
                                r
                                for r in execution_results
                                if r.get("status") == "success"
                            ]
                        ),
                        "intent": intent,
                    },
                    "streaming": True,
                },
            }

        except Exception as e:
            yield {
                "type": "error",
                "stage": "streaming_error",
                "message": f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "error": str(e),
                "progress": 0.0,
            }

    def _execute_step_streaming(
        self, step: Dict[str, Any], context: Dict[str, Any]
    ) -> Generator:
        """
        ê°œë³„ ë‹¨ê³„ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤í–‰

        Args:
            step: ì‹¤í–‰í•  ë‹¨ê³„
            context: ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸

        Returns:
            Generator ë˜ëŠ” None (ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
        """
        step_type = step.get("step_type", "general")
        required_tools = step.get("required_tools", [])

        # ìŠ¬ë¼ì´ë“œ ìƒì„± ë‹¨ê³„ì¸ ê²½ìš° ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
        if any(
            tool in required_tools
            for tool in ["slide_formatter", "format_slide", "slide_generator"]
        ):
            step_id = step.get("step_id", "unknown")

            try:
                # ReAct Executorë¥¼ í†µí•œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                executor = self._get_or_create_executor(step_id)

                # SlideFormatterì˜ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ í™œìš©
                if hasattr(executor, 'slide_formatter') and hasattr(
                    executor.slide_formatter, 'run_streaming'
                ):
                    # ìŠ¬ë¼ì´ë“œ ì½˜í…ì¸  ì¶”ì¶œ (ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜)
                    content = context.get("user_input", "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê°œìš”")

                    slide_inputs = {
                        "content": content,
                        "title": "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤",
                        "slide_type": "basic",
                        "subtitle": "",
                        "format": "json",
                    }

                    # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                    for chunk in executor.slide_formatter.run_streaming(slide_inputs):
                        yield chunk

                    return

            except Exception as e:
                yield {
                    "type": "error",
                    "stage": "slide_streaming_error",
                    "message": f"ìŠ¬ë¼ì´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}",
                    "error": str(e),
                }
                return

        # ë‹¤ë¥¸ ë‹¨ê³„ë“¤ì€ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ
        return None

    def _execute_single_step(
        self, step: Dict[str, Any], context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ê°œë³„ ë‹¨ê³„ë¥¼ ë¹„ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§)
        """
        step_id = step.get("step_id")
        step_type = step.get("step_type", "general")
        required_tools = step.get("required_tools", [])

        try:
            # ê¸°ì¡´ ì‹¤í–‰ ë¡œì§ ì‚¬ìš©
            if len(required_tools) == 1 and required_tools[0] in [
                "search_documents",
                "summarize_report",
                "get_tool_status",
            ]:
                # MCP ë„êµ¬ ì§ì ‘ ì‹¤í–‰
                tool_name = required_tools[0]

                if tool_name == "search_documents":
                    result = self.mcp_client.search_documents(
                        query=context.get("user_input", "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤"), top_k=5
                    )
                elif tool_name == "summarize_report":
                    result = self.mcp_client.summarize_report(
                        content=context.get("user_input", ""),
                        title="í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ë³´ê³ ì„œ",
                    )
                elif tool_name == "get_tool_status":
                    result = self.mcp_client.get_tool_status()

                if "error" in result:
                    raise Exception(result["error"])

                return {
                    "step_id": step_id,
                    "step_type": step_type,
                    "tool": tool_name,
                    "status": "success",
                    "result": result,
                    "final_result": str(result.get("result", result))[:500],
                }
            else:
                # ReAct ì‹¤í–‰ê¸°ë¥¼ í†µí•œ ì‹¤í–‰
                executor = self._get_or_create_executor(step_id)
                return executor.execute_step(step, context)

        except Exception as e:
            return {
                "step_id": step_id,
                "step_type": step_type,
                "status": "error",
                "error": str(e),
            }

    def _process_hybrid_execution(
        self,
        plan_result: Dict[str, Any],
        router_result: Dict[str, Any],
        user_input: str,
        start_time: float,
    ) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ë°©ì‹ ì²˜ë¦¬"""
        execution_steps = plan_result.get("execution_steps", [])
        dependency_graph = plan_result.get("dependency_graph", {})

        print(f"   â”” ì´ ì‹¤í–‰ ë‹¨ê³„: {len(execution_steps)}ê°œ")
        print(
            f"   â”” ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥: {len(dependency_graph.get('parallel_groups', []))}ê°œ ê·¸ë£¹"
        )

        # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (Plan & Execute + ReAct)
        print("\nğŸ”„ 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (Plan & Execute + ReAct)")
        execution_context = {
            "user_input": user_input,
            "intent": router_result.get("intent"),
            "key_entities": router_result.get("key_entities", []),
            "execution_steps": execution_steps,  # í‚¤ ì´ë¦„ ìˆ˜ì •!
            "execution_plan": execution_steps,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
            "dependency_graph": dependency_graph,
        }

        execution_results = self._execute_hybrid_workflow(execution_context)

        # 4ë‹¨ê³„: Trace Manager - ì „ì²´ ì¶”ë¡  ê³¼ì • ë¶„ì„
        print("\nğŸ“Š 4ë‹¨ê³„: Trace Manager - ì¶”ë¡  ê³¼ì • ë¶„ì„")
        trace_analysis = self._analyze_execution_trace(
            execution_results, execution_context
        )

        # 5ë‹¨ê³„: ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬ (í•„ìš”ì‹œ)
        if trace_analysis.get("final_assessment", {}).get("next_action") in [
            "retry",
            "revise",
        ]:
            print("\nğŸ”§ 5ë‹¨ê³„: ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬")
            recovery_result = self._handle_failure_recovery(
                execution_results, execution_context, trace_analysis
            )
            if recovery_result.get("recovery_status") == "success":
                execution_results = recovery_result.get(
                    "recovered_results", execution_results
                )

        # 6ë‹¨ê³„: Answer Agent - ìµœì¢… ì‘ë‹µ ìƒì„±
        print("\nâœ¨ 6ë‹¨ê³„: Answer Agent - ìµœì¢… ì‘ë‹µ ìƒì„±")
        final_response = self._generate_final_response(
            execution_results, trace_analysis, execution_context
        )

        # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total_time = time.time() - start_time

        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        final_result = {
            **final_response,
            "hybrid_execution_summary": {
                "total_execution_time": f"{total_time:.2f}ì´ˆ",
                "steps_executed": len(execution_results),
                "successful_steps": len(
                    [r for r in execution_results if r.get("status") == "success"]
                ),
                "reasoning_depth": trace_analysis.get("performance_metrics", {}).get(
                    "reasoning_depth", "medium"
                ),
                "overall_confidence": trace_analysis.get("final_assessment", {}).get(
                    "confidence", 0.5
                ),
            },
            "mcp_context": {
                **self.mcp_context,
                "processing_flow": [
                    f"Router: {router_result.get('intent', 'unknown')}",
                    f"Planner: {len(execution_steps)} steps planned",
                    f"Execution: {len(execution_results)} steps executed",
                    f"Trace Analysis: {trace_analysis.get('final_assessment', {}).get('workflow_status', 'unknown')}",
                    "Answer: completed",
                ],
                "status": "success",
                "hybrid_mode_used": True,
                "total_time": total_time,
            },
        }

        print(f"\nâœ… í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
        return final_result

    def _generate_direct_answer(self, user_input: str) -> str:
        """
        ì¼ë°˜ì ì¸ ëŒ€í™”ë¥¼ ìœ„í•œ ì§ì ‘ ì‘ë‹µ ìƒì„±

        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥

        Returns:
            str: ì§ì ‘ ì‘ë‹µ
        """
        # ê°„ë‹¨í•œ ì¸ì‚¬ë‚˜ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
        user_input_lower = user_input.lower()

        if any(
            greeting in user_input_lower
            for greeting in ["ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "ì‹œì‘"]
        ):
            return """
ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ 

ì €ëŠ” í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ê²ƒë“¤:**
â€¢ í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€
â€¢ ì •ì±… ë° ì»´í”Œë¼ì´ì–¸ìŠ¤ ê°€ì´ë“œ
â€¢ ìŠ¬ë¼ì´ë“œ ë° í”„ë ˆì  í…Œì´ì…˜ ìë£Œ ìƒì„±
â€¢ ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬ ë°©ì•ˆ ì œì‹œ

ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?
"""

        elif any(
            help_word in user_input_lower
            for help_word in ["ë„ì›€", "help", "ë­ í•  ìˆ˜", "ê¸°ëŠ¥"]
        ):
            return """
**í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ AI ì–´ì‹œìŠ¤í„´íŠ¸ ê¸°ëŠ¥ ì•ˆë‚´** ğŸ“š

ğŸ” **ì§ˆë¬¸ ì‘ë‹µ**
- í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ì •ì±…
- ì»´í”Œë¼ì´ì–¸ìŠ¤ ìš”êµ¬ì‚¬í•­
- ë³´ì•ˆ ê´€ë¦¬ ë°©ì•ˆ
- ëª¨ë‹ˆí„°ë§ ì „ëµ

ğŸ“Š **ìŠ¬ë¼ì´ë“œ ìƒì„±**
- í”„ë ˆì  í…Œì´ì…˜ ìë£Œ ì‘ì„±
- ê°œë… ì •ë¦¬ ìŠ¬ë¼ì´ë“œ
- ë¹„êµ ë¶„ì„ ìë£Œ

ì˜ˆì‹œ: "í´ë¼ìš°ë“œ ë³´ì•ˆ ì •ì±…ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”" ë˜ëŠ” "ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ìŠ¬ë¼ì´ë“œ ë§Œë“¤ì–´ì£¼ì„¸ìš”"
"""

        else:
            return """
í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ì™€ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ í•´ì£¼ì‹œë©´ ë” ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´:
â€¢ "í´ë¼ìš°ë“œ ë³´ì•ˆ ì •ì±…ì´ ë¬´ì—‡ì¸ê°€ìš”?"
â€¢ "ì»´í”Œë¼ì´ì–¸ìŠ¤ ê´€ë¦¬ ë°©ì•ˆ ìŠ¬ë¼ì´ë“œ ë§Œë“¤ì–´ì£¼ì„¸ìš”"
â€¢ "ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"

ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸ˜Š
"""

    def _get_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime

        return datetime.now().isoformat()

    def _execute_hybrid_workflow(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        execution_results = []
        execution_steps = context.get("execution_steps", [])
        dependency_graph = context.get("dependency_graph", {})

        for step in execution_steps:
            step_id = step.get("step_id")
            step_type = step.get("step_type", "general")
            required_tools = step.get("required_tools", [])
            description = step.get("description", "")

            print(f"\nğŸ” ì‹¤í–‰ ë‹¨ê³„ {step_id} ({step_type}): {description}")
            print(f"   ğŸ“‹ í•„ìš” ë„êµ¬: {required_tools}")

            try:
                # ë„êµ¬ ì´ë¦„ ì •ê·œí™” (PlannerAgentê°€ ì‚¬ìš©í•˜ëŠ” ì´ë¦„ì„ MCP ë„êµ¬ ì´ë¦„ìœ¼ë¡œ ë³€í™˜)
                normalized_tools = []
                for tool in required_tools:
                    if tool in [
                        "rag_retriever",
                        "search_documents",
                        "data_analyzer",
                        "content_validator",
                    ]:
                        # ë°ì´í„° ìˆ˜ì§‘/ë¶„ì„/ê²€ì¦ì€ ëª¨ë‘ RAG ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬
                        normalized_tools.append("search_documents")
                    elif tool in ["slide_formatter", "format_slide", "slide_generator"]:
                        # ìŠ¬ë¼ì´ë“œ ìƒì„±/í¬ë§·íŒ…ì€ LangChain Toolë¡œ ì²˜ë¦¬ (MCP ì œì™¸)
                        normalized_tools.append("slide_formatter")
                    elif tool in [
                        "report_summary",
                        "summarize_report",
                        "content_generator",
                    ]:
                        # ë³´ê³ ì„œ/ì½˜í…ì¸  ìƒì„±ì€ summarize_reportë¡œ ì²˜ë¦¬
                        normalized_tools.append("summarize_report")
                    elif tool in ["get_tool_status"]:
                        normalized_tools.append("get_tool_status")
                    else:
                        # ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ëŠ” ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
                        print(
                            f"   âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ '{tool}'ì„ search_documentsë¡œ ëŒ€ì²´"
                        )
                        normalized_tools.append("search_documents")

                # ë‹¨ì¼ MCP ë„êµ¬ ì§ì ‘ ì‹¤í–‰
                if len(normalized_tools) == 1 and normalized_tools[0] in [
                    "search_documents",
                    "format_slide",
                    "summarize_report",
                    "get_tool_status",
                ]:
                    tool_name = normalized_tools[0]
                    params = step.get("parameters", {})
                    print(f"   ğŸ”§ ì§ì ‘ MCP ë„êµ¬ ì‹¤í–‰: {tool_name}")

                    # ë‹¨ê³„ ìœ í˜•ì— ë”°ë¥¸ ê¸°ë³¸ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
                    if tool_name == "search_documents":
                        if not params:
                            # ë‹¨ê³„ ì„¤ëª…ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
                            description = step.get("description", "")
                            user_input = context.get("user_input", "")

                            if (
                                "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤" in description
                                or "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤" in user_input
                            ):
                                query = "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤"
                            elif "ë³´ì•ˆ" in description or "ë³´ì•ˆ" in user_input:
                                query = "í´ë¼ìš°ë“œ ë³´ì•ˆ"
                            elif "ì •ì±…" in description or "ì •ì±…" in user_input:
                                query = "í´ë¼ìš°ë“œ ì •ì±…"
                            else:
                                query = user_input[:50] or "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤"

                            params = {"query": query, "top_k": 5}
                        result = self.mcp_client.search_documents(**params)

                    elif tool_name == "format_slide":
                        if not params:
                            # ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ ë˜ëŠ” ê¸°ë³¸ê°’ ì„¤ì •
                            user_input = context.get("user_input", "")
                            if (
                                "ìŠ¬ë¼ì´ë“œ" in user_input
                                or "slide" in user_input.lower()
                            ):
                                content = (
                                    user_input.replace("ìŠ¬ë¼ì´ë“œ", "")
                                    .replace("slide", "")
                                    .strip()
                                )
                                content = content or "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê°œìš”"
                            else:
                                content = "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê°œìš”"

                            params = {
                                "content": content,
                                "title": "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤",
                                "slide_type": "basic",
                                "subtitle": "",
                                "format_type": "json",
                            }
                        result = self.mcp_client.format_slide(**params)

                    elif tool_name == "summarize_report":
                        if not params:
                            params = {
                                "content": context.get(
                                    "user_input", "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ë³´ê³ ì„œ"
                                ),
                                "title": "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ë³´ê³ ì„œ",
                                "summary_type": "executive",
                                "format_type": "html",
                            }
                        result = self.mcp_client.summarize_report(**params)

                    elif tool_name == "get_tool_status":
                        result = self.mcp_client.get_tool_status()

                    print(f"   ğŸ“Š MCP ë„êµ¬ ê²°ê³¼: {result.get('status', 'unknown')}")

                    if "error" in result:
                        print(f"   âš ï¸ MCP ë„êµ¬ ì˜¤ë¥˜: {result['error']}")
                        raise Exception(result["error"])

                    execution_results.append(
                        {
                            "step_id": step_id,
                            "step_type": step_type,
                            "tool": tool_name,
                            "status": "success",
                            "result": result,
                            "final_result": str(result.get("result", result))[:500],
                        }
                    )
                    print(f"   âœ… MCP ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ: {tool_name}")

                else:
                    # ReAct ì‹¤í–‰ê¸°ë¥¼ í†µí•œ ì‹¤í–‰ (ë³µí•© ë„êµ¬ ë˜ëŠ” ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš°)
                    print(f"   ğŸ¤– ReAct Executorë¡œ ì „ë‹¬: {normalized_tools}")
                    executor = self._get_or_create_executor(step_id)
                    result = executor.execute_step(step, context)
                    execution_results.append(result)
                    print(f"   âœ… ReAct ì‹¤í–‰ ì™„ë£Œ: {result.get('status', 'unknown')}")

            except Exception as e:
                print(f"   âŒ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
                execution_results.append(
                    {
                        "step_id": step_id,
                        "step_type": step_type,
                        "tool": required_tools[0] if required_tools else "unknown",
                        "status": "error",
                        "error": str(e),
                    }
                )

        return execution_results

    def _get_or_create_executor(self, executor_id: str) -> ReActExecutorAgent:
        """ReAct Executor ìƒì„± ë˜ëŠ” ê¸°ì¡´ ê²ƒ ë°˜í™˜"""
        if executor_id not in self.executor_pool:
            if len(self.executor_pool) >= self.max_executors:
                # í’€ì´ ê°€ë“ ì°¬ ê²½ìš° ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°
                oldest_key = next(iter(self.executor_pool))
                del self.executor_pool[oldest_key]

            self.executor_pool[executor_id] = ReActExecutorAgent(executor_id)

        return self.executor_pool[executor_id]

    def _analyze_execution_trace(
        self, execution_results: List[Dict[str, Any]], context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì „ì²´ ì‹¤í–‰ ì¶”ì  ë¶„ì„"""
        trace_input = {
            "execution_results": execution_results,
            "failed_steps": [
                r
                for r in execution_results
                if r.get("status") not in ["success", "partial_success"]
            ],
            "context": context,
        }

        return self.trace_manager(trace_input)

    def _handle_failure_recovery(
        self,
        execution_results: List[Dict[str, Any]],
        context: Dict[str, Any],
        trace_analysis: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬"""
        failed_steps = [
            r
            for r in execution_results
            if r.get("status") not in ["success", "partial_success"]
        ]

        if not failed_steps:
            return {"recovery_status": "no_recovery_needed"}

        # ê¸°ë³¸ ë³µêµ¬: ë‹¨ìˆœíˆ ì¬ì‹œë„ ê¶Œì¥
        return {
            "recovery_status": "completed",
            "recovery_strategy": "retry_recommended",
            "recovered_results": execution_results,
        }

    def _generate_final_response(
        self,
        execution_results: List[Dict[str, Any]],
        trace_analysis: Dict[str, Any],
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ìµœì¢… ì‘ë‹µ ìƒì„±"""
        # ì„±ê³µí•œ ê²°ê³¼ë“¤ì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ
        successful_results = [
            r for r in execution_results if r.get("status") == "success"
        ]

        if successful_results:
            latest_result = successful_results[-1]
            answer_content = latest_result.get("final_result", "")
        else:
            # ë¶€ë¶„ ì„±ê³µì´ë¼ë„ ì‚¬ìš©
            partial_results = [
                r for r in execution_results if r.get("status") == "partial_success"
            ]
            if partial_results:
                answer_content = partial_results[-1].get("final_result", "")
            else:
                answer_content = "ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        # Answer Agent ì…ë ¥ êµ¬ì„±
        answer_input = {
            "agent_type": "hybrid_execution",
            "answer_content": answer_content,
            "execution_results": execution_results,
            "reasoning_trace": self.reasoning_trace_logger.get_global_trace(),
            "trace_summary": trace_analysis.get("trace_summary", {}),
            "overall_confidence": trace_analysis.get("final_assessment", {}).get(
                "confidence", 0.5
            ),
            "source_type": "hybrid_react",
        }

        return self.answer_agent(answer_input)

    def _create_error_response(
        self, error_message: str, execution_time: float
    ) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±"""
        return {
            "final_answer": f"ì‹œìŠ¤í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message}\n\ní•˜ì´ë¸Œë¦¬ë“œ AI ì‹œìŠ¤í…œì´ ë³µêµ¬ë¥¼ ì‹œë„í–ˆì§€ë§Œ ì™„ì „í•œ ì²˜ë¦¬ê°€ ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
            "timestamp": self._get_timestamp(),
            "hybrid_execution_summary": {
                "total_execution_time": f"{execution_time:.2f}ì´ˆ",
                "steps_executed": 0,
                "successful_steps": 0,
                "reasoning_depth": "error",
                "overall_confidence": 0.0,
            },
            "mcp_context": {
                **self.mcp_context,
                "status": "error",
                "error_message": error_message,
                "hybrid_mode_used": False,
                "total_time": execution_time,
            },
        }

    def get_system_status(self) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸

        Returns:
            Dict[str, Any]: ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
        """
        return {
            "orchestrator": "hybrid_running",
            "agents": {
                "router": "initialized",
                "enhanced_planner": "initialized",
                "answer": "enhanced",
                "trace_manager": "initialized",
            },
            "react_executors": {
                "pool_size": len(self.executor_pool),
                "max_executors": self.max_executors,
                "active_executors": list(self.executor_pool.keys()),
            },
            "tools": {
                "reasoning_trace_logger": "active",
                "plan_revision_tool": "active",
                "state_manager": "active",
                "rag_retriever": "available",
                "slide_formatter": "available",
                "report_summary": "available",
            },
            "hybrid_features": {
                "parallel_execution": False,  # í–¥í›„ êµ¬í˜„
                "react_reasoning": True,
                "failure_recovery": True,
                "trace_analysis": True,
            },
            "mcp_context": self.mcp_context,
        }

    def clear_execution_state(self):
        """ì‹¤í–‰ ìƒíƒœ ì´ˆê¸°í™”"""
        self.executor_pool.clear()
        self.reasoning_trace_logger.clear_traces()
        self.plan_revision_tool.clear_history()
        self.state_manager.clear_all_states()
        print("ğŸ§¹ í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")
