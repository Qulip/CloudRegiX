from typing import Dict, Any, List, Generator
import time
import asyncio

from agents import (
    RouterAgent,
    PlannerAgent,
    AnswerAgent,
    ReActExecutorAgent,
    TraceManagerAgent,
)
from tools import (
    ReasoningTraceLogger,
    PlanRevisionTool,
    StateManager,
    SlideGeneratorTool,
)
from mcp_client import get_mcp_client

# langchain-mcp-adaptersë¥¼ ì‚¬ìš©í•œ MCP ë„êµ¬ ë¡œë”©
from langchain_mcp_adapters.client import MultiServerMCPClient


class CloudGovernanceOrchestrator:
    """
    í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ AI ì‹œìŠ¤í…œ í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
    Plan & Execute + ReAct í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ìž ìš”ì²­ ì²˜ë¦¬
    """

    def __init__(self):
        self.router_agent = RouterAgent()
        self.planner_agent = PlannerAgent()
        self.answer_agent = AnswerAgent()

        # ê¸°ì¡´ MCP í´ë¼ì´ì–¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)
        self.mcp_client = get_mcp_client()

        # LangChain Tool ì§ì ‘ ì‚¬ìš©
        self.slide_generator = SlideGeneratorTool()

        # ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì„± ìš”ì†Œë“¤
        self.trace_manager = TraceManagerAgent()
        self.reasoning_trace_logger = ReasoningTraceLogger()
        self.plan_revision_tool = PlanRevisionTool()
        self.state_manager = StateManager()

        # ReAct Executor Pool
        self.executor_pool = {}
        self.max_executors = 5

        # MCP ë„êµ¬ë“¤ì„ ìœ„í•œ MultiServerMCPClient ì„¤ì •
        self.mcp_multi_client = None
        self.mcp_tools = []
        self._initialize_mcp_tools()

        self.mcp_context = {
            "role": "hybrid_orchestrator",
            "function": "hybrid_workflow_coordination",
            "agents_initialized": True,
            "hybrid_mode": True,
            "mcp_tools_available": True,
            "langchain_tools_available": True,
        }

    def _initialize_mcp_tools(self):
        """MCP ë„êµ¬ë“¤ì„ ì´ˆê¸°í™”"""
        try:
            # MultiServerMCPClient ì„¤ì •
            self.mcp_multi_client = MultiServerMCPClient(
                {
                    "cloud_governance": {
                        "url": "http://localhost:8001/tools",
                        "transport": "streamable_http",
                    }
                }
            )
            print("âœ… MCP MultiServerMCPClient ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ MCP ë„êµ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.mcp_multi_client = None

    async def _get_mcp_tools(self):
        """MCP ë„êµ¬ë“¤ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if self.mcp_multi_client:
                tools = await self.mcp_multi_client.get_tools()
                return tools
            return []
        except Exception as e:
            print(f"âš ï¸ MCP ë„êµ¬ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            return []

    def _run_async_mcp_operation(self, coro):
        """ë¹„ë™ê¸° MCP ìž‘ì—…ì„ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰"""
        try:
            loop = asyncio.get_running_loop()
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìžˆìœ¼ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            import concurrent.futures
            import threading

            def run_in_thread():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(coro)
                finally:
                    new_loop.close()

            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_in_thread)
                return future.result()

        except RuntimeError:
            # ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(coro)
            finally:
                loop.close()

    def process_request(self, user_input: str) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ìž ìš”ì²­ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ

        Args:
            user_input (str): ì‚¬ìš©ìž ìž…ë ¥

        Returns:
            Dict[str, Any]: ìµœì¢… ì‘ë‹µ
        """
        start_time = time.time()

        try:
            print(f"ðŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì‹œìž‘: {user_input[:50]}...")

            # 1ë‹¨ê³„: Router Agent - ì˜ë„ ë¶„ì„
            print("\nðŸ“ 1ë‹¨ê³„: Router Agent - ì˜ë„ ë¶„ì„")
            router_result = self.router_agent({"user_input": user_input})
            print(f"   â”” Intent: {router_result.get('intent', 'unknown')}")

            # 2ë‹¨ê³„: Enhanced Planner Agent - í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
            print("\nðŸ“‹ 2ë‹¨ê³„: Enhanced Planner - í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½")
            planner_input = {**router_result, "user_input": user_input}
            plan_result = self.planner_agent(planner_input)

            return self._process_hybrid_execution(
                plan_result, router_result, user_input, start_time
            )

        except Exception as e:
            error_time = time.time() - start_time
            print(f"\nâŒ í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì˜¤ë¥˜: {str(e)}")
            return self._create_error_response(str(e), error_time)

    def process_request_streaming(
        self, user_input: str
    ) -> Generator[Dict[str, Any], None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ìž ìš”ì²­ ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ

        Args:
            user_input (str): ì‚¬ìš©ìž ìž…ë ¥

        Yields:
            Dict[str, Any]: ìŠ¤íŠ¸ë¦¬ë° ì²­í¬
        """
        start_time = time.time()

        try:
            print(f"\nðŸš€ [ORCHESTRATOR] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œìž‘: {user_input[:50]}...")

            yield {
                "type": "progress",
                "stage": "router_analysis",
                "message": "ì‚¬ìš©ìž ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ìžˆìŠµë‹ˆë‹¤...",
                "progress": 0.1,
            }

            # 1ë‹¨ê³„: Router Agent - ì˜ë„ ë¶„ì„
            print(f"\nðŸ“ [STEP 1] Router Agent ì‹¤í–‰ ì¤‘...")
            router_result = self.router_agent({"user_input": user_input})
            intent = router_result.get("intent", "unknown")
            print(f"   âœ… [ROUTER] ì˜ë„ ë¶„ì„ ì™„ë£Œ: {intent}")
            print(f"   ðŸ“Š [ROUTER] ì „ì²´ ê²°ê³¼: {router_result}")

            yield {
                "type": "progress",
                "stage": "planner_analysis",
                "message": f"ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ìžˆìŠµë‹ˆë‹¤... (ì˜ë„: {intent})",
                "progress": 0.2,
                "intent": intent,
            }

            # 2ë‹¨ê³„: Enhanced Planner Agent - í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
            print(f"\nðŸ“‹ [STEP 2] Planner Agent ì‹¤í–‰ ì¤‘...")
            planner_input = {**router_result, "user_input": user_input}
            print(f"   ðŸ“¥ [PLANNER] ìž…ë ¥ ë°ì´í„°: {planner_input}")
            plan_result = self.planner_agent(planner_input)
            print(f"   âœ… [PLANNER] ê³„íš ìˆ˜ë¦½ ì™„ë£Œ")
            print(f"   ðŸ“Š [PLANNER] ì „ì²´ ê²°ê³¼: {plan_result}")

            execution_steps = plan_result.get("execution_steps", [])
            dependency_graph = plan_result.get("dependency_graph", {})

            print(f"   ðŸ“‹ [PLANNER] ì‹¤í–‰ ë‹¨ê³„ ìˆ˜: {len(execution_steps)}")
            for i, step in enumerate(execution_steps):
                print(
                    f"      Step {i+1}: {step.get('step_id', 'unknown')} - {step.get('description', 'No description')[:50]}..."
                )

            yield {
                "type": "progress",
                "stage": "execution_start",
                "message": f"{len(execution_steps)}ê°œ ë‹¨ê³„ì˜ ì‹¤í–‰ì„ ì‹œìž‘í•©ë‹ˆë‹¤...",
                "progress": 0.3,
                "steps_count": len(execution_steps),
            }

            # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)
            print(f"\nâš¡ [STEP 3] í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ì‹œìž‘ ({len(execution_steps)}ê°œ ë‹¨ê³„)")
            execution_context = {
                "user_input": user_input,
                "intent": router_result.get("intent"),
                "key_entities": router_result.get("key_entities", []),
                "execution_steps": execution_steps,
                "execution_plan": execution_steps,
                "dependency_graph": dependency_graph,
            }

            # ë‹¨ê³„ë³„ ì‹¤í–‰ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬
            execution_results = []
            for i, step in enumerate(execution_steps):
                step_progress = 0.3 + (0.5 * (i + 1) / len(execution_steps))
                step_id = step.get("step_id", f"step_{i+1}")
                step_description = step.get("description", "Unknown step")
                required_tools = step.get("required_tools", [])

                print(f"\n   ðŸ”„ [STEP 3.{i+1}] ë‹¨ê³„ ì‹¤í–‰ ì‹œìž‘: {step_id}")
                print(f"      ðŸ“ ì„¤ëª…: {step_description}")
                print(f"      ðŸ› ï¸  í•„ìš” ë„êµ¬: {required_tools}")

                yield {
                    "type": "progress",
                    "stage": "step_execution",
                    "message": f"ë‹¨ê³„ {i+1}/{len(execution_steps)} ì‹¤í–‰ ì¤‘: {step_description}",
                    "progress": step_progress,
                    "current_step": step_id,
                }

                try:
                    # ë‹¨ê³„ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
                    print(f"      ðŸŽ¯ [EXECUTION] ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì‹œë„...")
                    step_result = self._execute_step_streaming(step, execution_context)

                    if step_result:
                        print(f"      âœ… [EXECUTION] ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì„±ê³µ")
                        final_result = None
                        chunk_count = 0

                        for chunk in step_result:
                            chunk_count += 1
                            print(
                                f"         ðŸ“¦ [CHUNK {chunk_count}] íƒ€ìž…: {chunk.get('type', 'unknown')}"
                            )

                            # ë„êµ¬ ì‹¤í–‰ ê³¼ì •ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ë‹¬
                            if chunk.get("type") in ["progress", "result", "error"]:
                                yield {
                                    "type": "tool_execution",
                                    "stage": chunk.get("stage", "unknown"),
                                    "message": chunk.get("message", ""),
                                    "progress": step_progress,
                                    "step_id": step_id,
                                    "chunk_data": chunk,
                                }

                                # ìµœì¢… ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ì €ìž¥
                        if chunk.get("type") == "result":
                            final_result = {
                                "step_id": step_id,
                                "status": "success",
                                "result": chunk.get("data", {}),
                                "final_result": str(chunk.get("data", {}))[:500],
                                "tool": (
                                    required_tools[0] if required_tools else "unknown"
                                ),
                            }
                            print(
                                f"         âœ… [RESULT] ìµœì¢… ê²°ê³¼ ì €ìž¥: {final_result['status']}"
                            )

                        if final_result:
                            execution_results.append(final_result)
                            # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ì— ê²°ê³¼ ì¶”ê°€ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡)
                            if "execution_results" not in execution_context:
                                execution_context["execution_results"] = []
                            execution_context["execution_results"].append(final_result)
                            print(
                                f"      âœ… [STEP 3.{i+1}] ì™„ë£Œ - ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ì €ìž¥ë¨"
                            )
                        else:
                            error_result = {
                                "step_id": step_id,
                                "status": "error",
                                "error": "ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ê²°ê³¼ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤",
                                "tool": (
                                    required_tools[0] if required_tools else "unknown"
                                ),
                            }
                            execution_results.append(error_result)
                            if "execution_results" not in execution_context:
                                execution_context["execution_results"] = []
                            execution_context["execution_results"].append(error_result)
                            print(f"      âŒ [STEP 3.{i+1}] ì‹¤íŒ¨ - ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ì—†ìŒ")
                    else:
                        # ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                        print(f"      ðŸ”„ [EXECUTION] ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì‹œë„...")
                        result = self._execute_single_step(step, execution_context)
                        execution_results.append(result)
                        # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ì— ê²°ê³¼ ì¶”ê°€
                        if "execution_results" not in execution_context:
                            execution_context["execution_results"] = []
                        execution_context["execution_results"].append(result)
                        print(
                            f"      âœ… [STEP 3.{i+1}] ì™„ë£Œ - ë¹„ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼: {result.get('status', 'unknown')}"
                        )

                except Exception as e:
                    error_result = {
                        "step_id": step_id,
                        "status": "error",
                        "error": str(e),
                        "tool": required_tools[0] if required_tools else "unknown",
                    }
                    execution_results.append(error_result)
                    # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ì— ê²°ê³¼ ì¶”ê°€
                    if "execution_results" not in execution_context:
                        execution_context["execution_results"] = []
                    execution_context["execution_results"].append(error_result)
                    print(f"      âŒ [STEP 3.{i+1}] ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

            print(
                f"\n   âœ… [STEP 3] í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ì™„ë£Œ: {len(execution_results)}ê°œ ê²°ê³¼"
            )
            for i, result in enumerate(execution_results):
                print(
                    f"      ê²°ê³¼ {i+1}: {result.get('step_id', 'unknown')} - {result.get('status', 'unknown')}"
                )

            yield {
                "type": "progress",
                "stage": "trace_analysis",
                "message": "ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìžˆìŠµë‹ˆë‹¤...",
                "progress": 0.8,
            }

            # 4ë‹¨ê³„: Trace Manager - ì „ì²´ ì¶”ë¡  ê³¼ì • ë¶„ì„
            print(f"\nðŸ“Š [STEP 4] Trace Manager ì‹¤í–‰ ì¤‘...")
            trace_analysis = self._analyze_execution_trace(
                execution_results, execution_context
            )
            print(
                f"   âœ… [TRACE] ë¶„ì„ ì™„ë£Œ: {trace_analysis.get('final_assessment', {}).get('workflow_status', 'unknown')}"
            )

            yield {
                "type": "progress",
                "stage": "final_response",
                "message": "ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ê³  ìžˆìŠµë‹ˆë‹¤...",
                "progress": 0.9,
            }

            # 5ë‹¨ê³„: Answer Agent - ìµœì¢… ì‘ë‹µ ìƒì„±
            print(f"\nâœ¨ [STEP 5] Answer Agent ì‹¤í–‰ ì¤‘...")
            final_response = self._generate_final_response(
                execution_results, trace_analysis, execution_context
            )
            print(f"   âœ… [ANSWER] ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ")

            total_time = time.time() - start_time

            # ìµœì¢… ê²°ê³¼
            final_data = {
                **final_response,
                "hybrid_execution_summary": {
                    "total_execution_time": f"{total_time:.2f}ì´ˆ",
                    "steps_executed": len(execution_results),
                    "successful_steps": len(
                        [r for r in execution_results if r.get("status") == "success"]
                    ),
                    "intent": intent,
                },
                "streaming": True,
            }

            print(f"\nðŸŽ‰ [ORCHESTRATOR] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
            print(
                f"   ðŸ“Š ì„±ê³µí•œ ë‹¨ê³„: {final_data['hybrid_execution_summary']['successful_steps']}/{final_data['hybrid_execution_summary']['steps_executed']}"
            )

            yield {
                "type": "result",
                "stage": "completed",
                "message": "ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "progress": 1.0,
                "data": final_data,
            }

        except Exception as e:
            print(f"\nâŒ [ORCHESTRATOR] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback

            traceback.print_exc()

            yield {
                "type": "error",
                "stage": "streaming_error",
                "message": f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "error": str(e),
                "progress": 0.0,
            }

    def _execute_step_streaming(
        self, step: Dict[str, Any], context: Dict[str, Any]
    ) -> Generator:
        """
        ê°œë³„ ë‹¨ê³„ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤í–‰

        Args:
            step: ì‹¤í–‰í•  ë‹¨ê³„
            context: ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸

        Returns:
            Generator ë˜ëŠ” None (ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
        """
        step_type = step.get("step_type", "general")
        required_tools = step.get("required_tools", [])
        step_id = step.get("step_id", "unknown")

        print(f"         ðŸŽ¯ [STREAMING] ë‹¨ê³„ íƒ€ìž…: {step_type}, ë„êµ¬: {required_tools}")

        # MCP ë„êµ¬ ì‹¤í–‰ì´ í•„ìš”í•œ ê²½ìš° (slide_draft í¬í•¨)
        if any(
            tool in required_tools
            for tool in ["rag_retriever", "report_summary", "slide_draft"]
        ):
            print(f"         ðŸ” [STREAMING] MCP ë„êµ¬ ì‹¤í–‰ í•„ìš” ê°ì§€")
            yield from self._execute_mcp_tools_streaming(step, context)
            return

        # ìŠ¬ë¼ì´ë“œ ìƒì„± ë‹¨ê³„ì¸ ê²½ìš° (LangChain Tool)
        if (
            any(
                tool in required_tools
                for tool in ["slide_formatter", "format_slide", "slide_generator"]
            )
            or step_type == "generating"
        ):
            print(f"         ðŸ“Š [STREAMING] ìŠ¬ë¼ì´ë“œ ìƒì„± ë„êµ¬ ê°ì§€")
            yield from self._execute_slide_generation_streaming(step, context)
            return

        # ReAct Executorê°€ í•„ìš”í•œ ë³µìž¡í•œ ë‹¨ê³„ (analysis, validation ë“±)
        if step_type in ["analysis", "validation"] and len(required_tools) > 1:
            print(f"         ðŸ¤– [STREAMING] ReAct Executor í•„ìš”")
            yield from self._execute_react_streaming(step, context)
            return

        # drafting ë‹¨ê³„ ì²˜ë¦¬ (slide_draft ë„êµ¬ ì‚¬ìš©)
        if step_type == "drafting":
            print(f"         ðŸ“ [STREAMING] ì´ˆì•ˆ ìž‘ì„± ë‹¨ê³„ ê°ì§€")
            yield from self._execute_mcp_tools_streaming(step, context)
            return

        # data_collection ë‹¨ê³„ ì²˜ë¦¬
        if step_type == "data_collection":
            print(f"         ðŸ“Š [STREAMING] ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ ê°ì§€")
            yield from self._execute_mcp_tools_streaming(step, context)
            return

        # ê¸°ë³¸ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ
        print(f"         âŒ [STREAMING] ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì› ë‹¨ê³„: {step_type}")
        return None

    def _execute_mcp_tools_streaming(
        self, step: Dict[str, Any], context: Dict[str, Any]
    ) -> Generator:
        """MCP ë„êµ¬ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤í–‰"""
        step_id = step.get("step_id", "unknown")
        step_type = step.get("step_type", "general")
        required_tools = step.get("required_tools", [])

        print(f"         ðŸ”§ [MCP] ë¹„ë™ê¸° MCP ë„êµ¬ ì‹¤í–‰ ì‹œìž‘...")

        yield {
            "type": "progress",
            "stage": "mcp_tool_execution",
            "message": "MCP ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³  ìžˆìŠµë‹ˆë‹¤...",
            "progress": 0.3,
        }

        try:
            # ë‹¨ê³„ë³„ ì‹¤í–‰
            result = self._execute_single_step(step, context)

            yield {
                "type": "result",
                "stage": "mcp_completed",
                "message": "MCP ë„êµ¬ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "progress": 1.0,
                "data": result,
            }

        except Exception as e:
            print(f"         âŒ [MCP] ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            yield {
                "type": "error",
                "stage": "mcp_failed",
                "message": f"MCP ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}",
                "progress": 0.0,
                "error": str(e),
            }

    def _execute_slide_generation_streaming(
        self, step: Dict[str, Any], context: Dict[str, Any]
    ) -> Generator:
        """ìŠ¬ë¼ì´ë“œ ìƒì„±ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤í–‰"""
        step_id = step.get("step_id", "unknown")

        print(f"         ðŸŽ¨ [SLIDE] ìŠ¬ë¼ì´ë“œ ìƒì„± ì‹œìž‘...")

        yield {
            "type": "progress",
            "stage": "analyzing_draft",
            "message": "ìŠ¬ë¼ì´ë“œ ì´ˆì•ˆ ë¶„ì„ ì¤‘...",
            "progress": 0.2,
        }

        yield {
            "type": "progress",
            "stage": "generating_structure",
            "message": "ìŠ¬ë¼ì´ë“œ êµ¬ì¡° ìƒì„± ì¤‘...",
            "progress": 0.5,
        }

        yield {
            "type": "progress",
            "stage": "formatting_html",
            "message": "HTML í˜•ì‹ ë³€í™˜ ì¤‘...",
            "progress": 0.8,
        }

        try:
            # ì‹¤ì œ ìŠ¬ë¼ì´ë“œ ìƒì„± ì‹¤í–‰
            result = self._execute_single_step(step, context)

            yield {
                "type": "result",
                "stage": "completed",
                "message": "ìŠ¬ë¼ì´ë“œ ìƒì„± ì™„ë£Œ",
                "progress": 1.0,
                "data": result,
            }

        except Exception as e:
            print(f"         âŒ [SLIDE] ìƒì„± ì‹¤íŒ¨: {str(e)}")
            yield {
                "type": "error",
                "stage": "slide_failed",
                "message": f"ìŠ¬ë¼ì´ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                "progress": 0.0,
                "error": str(e),
            }

    def _execute_react_streaming(
        self, step: Dict[str, Any], context: Dict[str, Any]
    ) -> Generator:
        """ReAct Executorë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤í–‰"""
        step_id = step.get("step_id", "unknown")

        print(f"         ðŸ¤– [REACT] ReAct Executor ì‹¤í–‰ ì‹œìž‘...")

        yield {
            "type": "progress",
            "stage": "react_thinking",
            "message": "ì¶”ë¡  ê³¼ì •ì„ ì‹¤í–‰í•˜ê³  ìžˆìŠµë‹ˆë‹¤...",
            "progress": 0.3,
        }

        try:
            # ReAct Executor ì‹¤í–‰
            result = self._execute_single_step(step, context)

            yield {
                "type": "result",
                "stage": "react_completed",
                "message": "ReAct ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "progress": 1.0,
                "data": result,
            }

        except Exception as e:
            print(f"         âŒ [REACT] ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            yield {
                "type": "error",
                "stage": "react_failed",
                "message": f"ReAct ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}",
                "progress": 0.0,
                "error": str(e),
            }

    def _execute_single_step(
        self, step: Dict[str, Any], context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ê°œë³„ ë‹¨ê³„ ì‹¤í–‰ (MCP ë„êµ¬ ì§ì ‘ ì‹¤í–‰ ë˜ëŠ” ReAct ì‹¤í–‰ê¸° ì‚¬ìš©)

        Args:
            step: ì‹¤í–‰í•  ë‹¨ê³„
            context: ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸

        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        step_id = step.get("step_id", "unknown")
        step_type = step.get("step_type", "general")
        required_tools = step.get("required_tools", [])
        step_description = step.get("description", "")

        print(f"      ðŸ”„ [SINGLE_STEP] ë‹¨ê³„ ì‹¤í–‰ ì‹œìž‘: {step_id}")
        print(f"         ðŸ“ ì„¤ëª…: {step_description}")
        print(f"         ðŸ› ï¸  ë„êµ¬: {required_tools}")
        print(f"         ðŸ“Š íƒ€ìž…: {step_type}")

        try:
            # ë„êµ¬ ì´ë¦„ ì •ê·œí™”
            print(f"         ðŸ”§ [NORMALIZE] ë„êµ¬ ì´ë¦„ ì •ê·œí™” ì‹œìž‘...")
            normalized_tools = []
            for tool in required_tools:
                if tool in [
                    "rag_retriever",
                    "search_documents",
                    "data_analyzer",
                    "content_validator",
                ]:
                    normalized_tools.append("search_documents")
                    print(f"            âœ… '{tool}' â†’ 'search_documents'")
                elif tool in ["slide_formatter", "format_slide", "slide_generator"]:
                    # ìŠ¬ë¼ì´ë“œ ìƒì„±ì€ LangChain Toolë¡œ ì§ì ‘ ì²˜ë¦¬
                    normalized_tools.append("slide_generator_langchain")
                    print(f"            âœ… '{tool}' â†’ 'slide_generator_langchain'")
                elif tool in ["slide_draft", "create_slide_draft"]:
                    # ìŠ¬ë¼ì´ë“œ ì´ˆì•ˆ ìƒì„±ì€ MCP ë„êµ¬ë¡œ ì²˜ë¦¬
                    normalized_tools.append("create_slide_draft")
                    print(f"            âœ… '{tool}' â†’ 'create_slide_draft'")
                elif tool in [
                    "report_summary",
                    "summarize_report",
                    "content_generator",
                ]:
                    normalized_tools.append("summarize_report")
                    print(f"            âœ… '{tool}' â†’ 'summarize_report'")
                elif tool in ["get_tool_status"]:
                    normalized_tools.append("get_tool_status")
                    print(f"            âœ… '{tool}' â†’ 'get_tool_status'")
                else:
                    normalized_tools.append("search_documents")
                    print(f"            âš ï¸ '{tool}' â†’ 'search_documents' (ê¸°ë³¸ê°’)")

            print(f"         ðŸ“‹ [NORMALIZE] ì •ê·œí™”ëœ ë„êµ¬: {normalized_tools}")

            # LangChain Tool ì§ì ‘ ì‹¤í–‰ (ìŠ¬ë¼ì´ë“œ ìƒì„±)
            if "slide_generator_langchain" in normalized_tools:
                print(f"         ðŸŽ¨ [LANGCHAIN] SlideGenerator ë„êµ¬ ì§ì ‘ ì‹¤í–‰")

                # ì‚¬ìš©ìž ìž…ë ¥ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
                user_input = context.get("user_input", "")

                # ì´ì „ ë‹¨ê³„ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ì™€ ìŠ¬ë¼ì´ë“œ ì´ˆì•ˆ ê°€ì ¸ì˜¤ê¸°
                search_results = []
                slide_draft = {
                    "title": "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤",
                    "bullets": [
                        "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê°œìš”",
                        "ì£¼ìš” êµ¬ì„± ìš”ì†Œ",
                        "êµ¬í˜„ ë°©ì•ˆ",
                        "ê¸°ëŒ€ íš¨ê³¼",
                    ],
                    "notes": "ì‚¬ìš©ìž ìš”ì²­ ê¸°ë°˜ ìŠ¬ë¼ì´ë“œ",
                }

                # ì‹¤í–‰ ê²°ê³¼ì—ì„œ ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤ ìˆ˜ì§‘
                execution_results = context.get("execution_results", [])
                print(
                    f"            ðŸ“‹ [LANGCHAIN] ì´ì „ ë‹¨ê³„ ê²°ê³¼ ìˆ˜: {len(execution_results)}"
                )

                for prev_result in execution_results:
                    result_tool = prev_result.get("tool", "")
                    result_data = prev_result.get("result", {})

                    # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
                    if result_tool == "search_documents":
                        try:
                            if isinstance(result_data, str):
                                import json

                                result_data = json.loads(result_data)
                            search_results = result_data.get("results", [])
                            print(
                                f"            âœ… [LANGCHAIN] ê²€ìƒ‰ ê²°ê³¼ íšë“: {len(search_results)}ê°œ"
                            )
                        except Exception as e:
                            print(f"            âš ï¸ [LANGCHAIN] ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")

                    # ìŠ¬ë¼ì´ë“œ ì´ˆì•ˆ ì¶”ì¶œ
                    elif result_tool == "create_slide_draft":
                        try:
                            if isinstance(result_data, str):
                                import json

                                result_data = json.loads(result_data)
                            slide_draft = result_data.get("draft", slide_draft)
                            print(
                                f"            âœ… [LANGCHAIN] ìŠ¬ë¼ì´ë“œ ì´ˆì•ˆ íšë“: {slide_draft.get('title', 'No title')}"
                            )
                        except Exception as e:
                            print(
                                f"            âš ï¸ [LANGCHAIN] ìŠ¬ë¼ì´ë“œ ì´ˆì•ˆ íŒŒì‹± ì‹¤íŒ¨: {e}"
                            )

                slide_inputs = {
                    "slide_draft": slide_draft,
                    "search_results": search_results,
                    "user_input": user_input,
                    "slide_type": "basic",
                    "format_type": "html",
                }

                print(f"            ðŸ“‹ [LANGCHAIN] ìµœì¢… ìŠ¬ë¼ì´ë“œ ìž…ë ¥:")
                print(
                    f"                - ì´ˆì•ˆ ì œëª©: {slide_draft.get('title', 'No title')}"
                )
                print(f"                - ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
                print(f"                - ì‚¬ìš©ìž ìž…ë ¥: {user_input[:50]}...")
                print(f"            â–¶ï¸  [LANGCHAIN] SlideGenerator ì‹¤í–‰ ì¤‘...")

                result = self.slide_generator.run(slide_inputs)

                print(f"            âœ… [LANGCHAIN] SlideGenerator ì‹¤í–‰ ì™„ë£Œ")
                print(f"            ðŸ“Š [LANGCHAIN] ê²°ê³¼ íƒ€ìž…: {type(result)}")

                return {
                    "step_id": step_id,
                    "step_type": step_type,
                    "tool": "slide_generator_langchain",
                    "status": "success",
                    "result": result,
                    "final_result": str(result.get("html", ""))[:500],
                }

            # MCP ë„êµ¬ ì‹¤í–‰ (ë‹¨ì¼ ë„êµ¬)
            elif len(normalized_tools) == 1 and normalized_tools[0] in [
                "search_documents",
                "summarize_report",
                "create_slide_draft",
                "get_tool_status",
            ]:
                tool_name = normalized_tools[0]
                print(f"         ðŸ”§ [MCP] MCP ë„êµ¬ ì‹¤í–‰: {tool_name}")

                # MCP ë„êµ¬ ì‹¤í–‰ì„ ìœ„í•œ ë¹„ë™ê¸° í•¨ìˆ˜
                async def execute_mcp_tool():
                    try:
                        print(f"            ðŸ”— [MCP] MCP í´ë¼ì´ì–¸íŠ¸ í™•ì¸...")
                        if not self.mcp_multi_client:
                            raise Exception("MCP í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

                        print(f"            ðŸ“‹ [MCP] MCP ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
                        # MCP ë„êµ¬ë“¤ ê°€ì ¸ì˜¤ê¸°
                        tools = await self._get_mcp_tools()
                        print(f"            ðŸ“Š [MCP] ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ìˆ˜: {len(tools)}")

                        if tools:
                            tool_names = [tool.name for tool in tools]
                            print(f"            ðŸ“‹ [MCP] ë„êµ¬ ëª©ë¡: {tool_names}")

                        # í•´ë‹¹ ë„êµ¬ ì°¾ê¸°
                        target_tool = None
                        for tool in tools:
                            if tool.name == tool_name:
                                target_tool = tool
                                break

                        if not target_tool:
                            available_tools = (
                                [tool.name for tool in tools] if tools else []
                            )
                            raise Exception(
                                f"MCP ë„êµ¬ '{tool_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {available_tools}"
                            )

                        print(
                            f"            âœ… [MCP] ëŒ€ìƒ ë„êµ¬ ë°œê²¬: {target_tool.name}"
                        )

                        # ë„êµ¬ë³„ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
                        if tool_name == "search_documents":
                            params = step.get("parameters", {})
                            if not params:
                                description = step.get("description", "")
                                user_input = context.get("user_input", "")

                                # ê²€ìƒ‰ ì¿¼ë¦¬ ê²°ì •
                                if (
                                    "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤" in description
                                    or "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤" in user_input
                                ):
                                    query = "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤"
                                elif "ë³´ì•ˆ" in description or "ë³´ì•ˆ" in user_input:
                                    query = "í´ë¼ìš°ë“œ ë³´ì•ˆ"
                                elif "ì •ì±…" in description or "ì •ì±…" in user_input:
                                    query = "í´ë¼ìš°ë“œ ì •ì±…"
                                else:
                                    query = user_input[:50] or "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤"

                                params = {"query": query, "top_k": 5}

                            print(
                                f"            ðŸ“‹ [MCP] search_documents ë§¤ê°œë³€ìˆ˜: {params}"
                            )
                            print(f"            â–¶ï¸  [MCP] search_documents ì‹¤í–‰ ì¤‘...")
                            result = await target_tool.ainvoke(params)
                            print(f"            âœ… [MCP] search_documents ì‹¤í–‰ ì™„ë£Œ")

                        elif tool_name == "create_slide_draft":
                            # ì´ì „ ë‹¨ê³„ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                            search_results = []
                            for prev_result in context.get("execution_results", []):
                                if prev_result.get("tool") == "search_documents":
                                    try:
                                        result_data = prev_result.get("result", {})
                                        if isinstance(result_data, str):
                                            import json

                                            result_data = json.loads(result_data)
                                        search_results = result_data.get("results", [])
                                        break
                                    except:
                                        pass

                            params = {
                                "search_results": search_results,
                                "user_input": context.get("user_input", ""),
                                "slide_type": "basic",
                                "title": "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤",
                            }

                            print(
                                f"            ðŸ“‹ [MCP] create_slide_draft ë§¤ê°œë³€ìˆ˜: {len(search_results)}ê°œ ê²€ìƒ‰ ê²°ê³¼"
                            )
                            print(f"            â–¶ï¸  [MCP] create_slide_draft ì‹¤í–‰ ì¤‘...")
                            result = await target_tool.ainvoke(params)
                            print(f"            âœ… [MCP] create_slide_draft ì‹¤í–‰ ì™„ë£Œ")

                        elif tool_name == "summarize_report":
                            params = step.get("parameters", {})
                            if not params:
                                params = {
                                    "content": context.get(
                                        "user_input", "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ë³´ê³ ì„œ"
                                    ),
                                    "title": "í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ë³´ê³ ì„œ",
                                    "summary_type": "executive",
                                    "format_type": "html",
                                }

                            print(
                                f"            ðŸ“‹ [MCP] summarize_report ë§¤ê°œë³€ìˆ˜: {params}"
                            )
                            print(f"            â–¶ï¸  [MCP] summarize_report ì‹¤í–‰ ì¤‘...")
                            result = await target_tool.ainvoke(params)
                            print(f"            âœ… [MCP] summarize_report ì‹¤í–‰ ì™„ë£Œ")

                        elif tool_name == "get_tool_status":
                            print(f"            â–¶ï¸  [MCP] get_tool_status ì‹¤í–‰ ì¤‘...")
                            result = await target_tool.ainvoke({})
                            print(f"            âœ… [MCP] get_tool_status ì‹¤í–‰ ì™„ë£Œ")

                        print(f"            ðŸ“Š [MCP] ê²°ê³¼ íƒ€ìž…: {type(result)}")
                        print(
                            f"            ðŸ“‹ [MCP] ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {str(result)[:200]}..."
                        )

                        return {
                            "step_id": step_id,
                            "step_type": step_type,
                            "tool": tool_name,
                            "status": "success",
                            "result": result,
                            "final_result": str(result)[:500],
                        }

                    except Exception as e:
                        print(f"            âŒ [MCP] ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
                        import traceback

                        traceback.print_exc()
                        return {
                            "step_id": step_id,
                            "step_type": step_type,
                            "tool": tool_name,
                            "status": "error",
                            "error": str(e),
                        }

                # ë¹„ë™ê¸° MCP ë„êµ¬ ì‹¤í–‰
                print(f"            ðŸ”„ [MCP] ë¹„ë™ê¸° ì‹¤í–‰ ì‹œìž‘...")
                result = self._run_async_mcp_operation(execute_mcp_tool())
                print(
                    f"            âœ… [MCP] ë¹„ë™ê¸° ì‹¤í–‰ ì™„ë£Œ: {result.get('status', 'unknown')}"
                )
                return result

            else:
                # ReAct ì‹¤í–‰ê¸°ë¥¼ í†µí•œ ì‹¤í–‰ (ë³µí•© ë„êµ¬ ë˜ëŠ” ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš°)
                print(f"         ðŸ¤– [REACT] ReAct Executorë¡œ ì „ë‹¬: {normalized_tools}")
                executor = self._get_or_create_executor(step_id)
                print(f"            ðŸ“‹ [REACT] Executor ID: {step_id}")
                print(f"            â–¶ï¸  [REACT] ì‹¤í–‰ ì¤‘...")
                result = executor.execute_step(step, context)
                print(
                    f"            âœ… [REACT] ì‹¤í–‰ ì™„ë£Œ: {result.get('status', 'unknown')}"
                )
                return result

        except Exception as e:
            print(f"         âŒ [SINGLE_STEP] ë‹¨ê³„ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            import traceback

            traceback.print_exc()
            return {
                "step_id": step_id,
                "step_type": step_type,
                "tool": required_tools[0] if required_tools else "unknown",
                "status": "error",
                "error": str(e),
            }

    def _process_hybrid_execution(
        self,
        plan_result: Dict[str, Any],
        router_result: Dict[str, Any],
        user_input: str,
        start_time: float,
    ) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ë°©ì‹ ì²˜ë¦¬"""
        execution_steps = plan_result.get("execution_steps", [])
        dependency_graph = plan_result.get("dependency_graph", {})

        print(f"   â”” ì´ ì‹¤í–‰ ë‹¨ê³„: {len(execution_steps)}ê°œ")
        print(
            f"   â”” ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥: {len(dependency_graph.get('parallel_groups', []))}ê°œ ê·¸ë£¹"
        )

        # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (Plan & Execute + ReAct)
        print("\nðŸ”„ 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (Plan & Execute + ReAct)")
        execution_context = {
            "user_input": user_input,
            "intent": router_result.get("intent"),
            "key_entities": router_result.get("key_entities", []),
            "execution_steps": execution_steps,  # í‚¤ ì´ë¦„ ìˆ˜ì •!
            "execution_plan": execution_steps,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
            "dependency_graph": dependency_graph,
        }

        # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ (ë‹¨ê³„ë³„ ì‹¤í–‰)
        print("\nâš¡ 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰")
        execution_results = []

        for step in execution_steps:
            try:
                # ë‹¨ê³„ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì› í™•ì¸)
                step_result = self._execute_step_streaming(step, execution_context)
                if step_result:
                    # ìŠ¤íŠ¸ë¦¬ë° ì§€ì›í•˜ëŠ” ê²½ìš° - ë§ˆì§€ë§‰ ê²°ê³¼ë§Œ ìˆ˜ì§‘
                    final_result = None
                    for chunk in step_result:
                        if chunk.get("type") == "result":
                            final_result = {
                                "step_id": step.get("step_id"),
                                "status": "success",
                                "result": chunk.get("data", {}),
                                "final_result": str(chunk.get("data", {}))[:500],
                            }

                    if final_result:
                        execution_results.append(final_result)
                    else:
                        # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ
                        execution_results.append(
                            {
                                "step_id": step.get("step_id"),
                                "status": "error",
                                "error": "ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ê²°ê³¼ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤",
                            }
                        )
                else:
                    # ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                    result = self._execute_single_step(step, execution_context)
                    execution_results.append(result)

            except Exception as e:
                execution_results.append(
                    {
                        "step_id": step.get("step_id"),
                        "status": "error",
                        "error": str(e),
                    }
                )

        print(f"   âœ… ì‹¤í–‰ ì™„ë£Œ: {len(execution_results)}ê°œ ë‹¨ê³„")

        # 4ë‹¨ê³„: Trace Manager - ì „ì²´ ì¶”ë¡  ê³¼ì • ë¶„ì„
        print("\nðŸ“Š 4ë‹¨ê³„: Trace Manager - ì¶”ë¡  ê³¼ì • ë¶„ì„")
        trace_analysis = self._analyze_execution_trace(
            execution_results, execution_context
        )

        # 5ë‹¨ê³„: ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬ (í•„ìš”ì‹œ)
        if trace_analysis.get("final_assessment", {}).get("next_action") in [
            "retry",
            "revise",
        ]:
            print("\nðŸ”§ 5ë‹¨ê³„: ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬")
            recovery_result = self._handle_failure_recovery(
                execution_results, execution_context, trace_analysis
            )
            if recovery_result.get("recovery_status") == "success":
                execution_results = recovery_result.get(
                    "recovered_results", execution_results
                )

        # 6ë‹¨ê³„: Answer Agent - ìµœì¢… ì‘ë‹µ ìƒì„±
        print("\nâœ¨ 6ë‹¨ê³„: Answer Agent - ìµœì¢… ì‘ë‹µ ìƒì„±")
        final_response = self._generate_final_response(
            execution_results, trace_analysis, execution_context
        )

        # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total_time = time.time() - start_time

        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        final_result = {
            **final_response,
            "hybrid_execution_summary": {
                "total_execution_time": f"{total_time:.2f}ì´ˆ",
                "steps_executed": len(execution_results),
                "successful_steps": len(
                    [r for r in execution_results if r.get("status") == "success"]
                ),
                "reasoning_depth": trace_analysis.get("performance_metrics", {}).get(
                    "reasoning_depth", "medium"
                ),
                "overall_confidence": trace_analysis.get("final_assessment", {}).get(
                    "confidence", 0.5
                ),
            },
            "mcp_context": {
                **self.mcp_context,
                "processing_flow": [
                    f"Router: {router_result.get('intent', 'unknown')}",
                    f"Planner: {len(execution_steps)} steps planned",
                    f"Execution: {len(execution_results)} steps executed",
                    f"Trace Analysis: {trace_analysis.get('final_assessment', {}).get('workflow_status', 'unknown')}",
                    "Answer: completed",
                ],
                "status": "success",
                "hybrid_mode_used": True,
                "total_time": total_time,
            },
        }

        print(f"\nâœ… í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
        return final_result

    def _generate_direct_answer(self, user_input: str) -> str:
        """
        ì¼ë°˜ì ì¸ ëŒ€í™”ë¥¼ ìœ„í•œ ì§ì ‘ ì‘ë‹µ ìƒì„±

        Args:
            user_input (str): ì‚¬ìš©ìž ìž…ë ¥

        Returns:
            str: ì§ì ‘ ì‘ë‹µ
        """
        # ê°„ë‹¨í•œ ì¸ì‚¬ë‚˜ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
        user_input_lower = user_input.lower()

        if any(
            greeting in user_input_lower
            for greeting in ["ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "ì‹œìž‘"]
        ):
            return """
ì•ˆë…•í•˜ì„¸ìš”! ðŸ‘‹ 

ì €ëŠ” í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤.

**ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìžˆëŠ” ê²ƒë“¤:**
â€¢ í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€
â€¢ ì •ì±… ë° ì»´í”Œë¼ì´ì–¸ìŠ¤ ê°€ì´ë“œ
â€¢ ìŠ¬ë¼ì´ë“œ ë° í”„ë ˆì  í…Œì´ì…˜ ìžë£Œ ìƒì„±
â€¢ ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬ ë°©ì•ˆ ì œì‹œ

ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?
"""

        elif any(
            help_word in user_input_lower
            for help_word in ["ë„ì›€", "help", "ë­ í•  ìˆ˜", "ê¸°ëŠ¥"]
        ):
            return """
**í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ AI ì–´ì‹œìŠ¤í„´íŠ¸ ê¸°ëŠ¥ ì•ˆë‚´** ðŸ“š

ðŸ” **ì§ˆë¬¸ ì‘ë‹µ**
- í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ ì •ì±…
- ì»´í”Œë¼ì´ì–¸ìŠ¤ ìš”êµ¬ì‚¬í•­
- ë³´ì•ˆ ê´€ë¦¬ ë°©ì•ˆ
- ëª¨ë‹ˆí„°ë§ ì „ëžµ

ðŸ“Š **ìŠ¬ë¼ì´ë“œ ìƒì„±**
- í”„ë ˆì  í…Œì´ì…˜ ìžë£Œ ìž‘ì„±
- ê°œë… ì •ë¦¬ ìŠ¬ë¼ì´ë“œ
- ë¹„êµ ë¶„ì„ ìžë£Œ

ì˜ˆì‹œ: "í´ë¼ìš°ë“œ ë³´ì•ˆ ì •ì±…ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”" ë˜ëŠ” "ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ìŠ¬ë¼ì´ë“œ ë§Œë“¤ì–´ì£¼ì„¸ìš”"
"""

        else:
            return """
í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ì™€ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ í•´ì£¼ì‹œë©´ ë” ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´:
â€¢ "í´ë¼ìš°ë“œ ë³´ì•ˆ ì •ì±…ì´ ë¬´ì—‡ì¸ê°€ìš”?"
â€¢ "ì»´í”Œë¼ì´ì–¸ìŠ¤ ê´€ë¦¬ ë°©ì•ˆ ìŠ¬ë¼ì´ë“œ ë§Œë“¤ì–´ì£¼ì„¸ìš”"
â€¢ "ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"

ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”! ðŸ˜Š
"""

    def _get_timestamp(self) -> str:
        """í˜„ìž¬ íƒ€ìž„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime

        return datetime.now().isoformat()

    def _get_or_create_executor(self, executor_id: str) -> ReActExecutorAgent:
        """ReAct Executor ìƒì„± ë˜ëŠ” ê¸°ì¡´ ê²ƒ ë°˜í™˜"""
        if executor_id not in self.executor_pool:
            if len(self.executor_pool) >= self.max_executors:
                # í’€ì´ ê°€ë“ ì°¬ ê²½ìš° ê°€ìž¥ ì˜¤ëž˜ëœ ê²ƒ ì œê±°
                oldest_key = next(iter(self.executor_pool))
                del self.executor_pool[oldest_key]

            self.executor_pool[executor_id] = ReActExecutorAgent(executor_id)

        return self.executor_pool[executor_id]

    def _analyze_execution_trace(
        self, execution_results: List[Dict[str, Any]], context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì „ì²´ ì‹¤í–‰ ì¶”ì  ë¶„ì„"""
        trace_input = {
            "execution_results": execution_results,
            "failed_steps": [
                r
                for r in execution_results
                if r.get("status") not in ["success", "partial_success"]
            ],
            "context": context,
        }

        return self.trace_manager(trace_input)

    def _handle_failure_recovery(
        self,
        execution_results: List[Dict[str, Any]],
        context: Dict[str, Any],
        trace_analysis: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬"""
        failed_steps = [
            r
            for r in execution_results
            if r.get("status") not in ["success", "partial_success"]
        ]

        if not failed_steps:
            return {"recovery_status": "no_recovery_needed"}

        # ê¸°ë³¸ ë³µêµ¬: ë‹¨ìˆœížˆ ìž¬ì‹œë„ ê¶Œìž¥
        return {
            "recovery_status": "completed",
            "recovery_strategy": "retry_recommended",
            "recovered_results": execution_results,
        }

    def _generate_final_response(
        self,
        execution_results: List[Dict[str, Any]],
        trace_analysis: Dict[str, Any],
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ìµœì¢… ì‘ë‹µ ìƒì„±"""
        # ì„±ê³µí•œ ê²°ê³¼ë“¤ì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ
        successful_results = [
            r for r in execution_results if r.get("status") == "success"
        ]

        if successful_results:
            latest_result = successful_results[-1]
            answer_content = latest_result.get("final_result", "")
        else:
            # ë¶€ë¶„ ì„±ê³µì´ë¼ë„ ì‚¬ìš©
            partial_results = [
                r for r in execution_results if r.get("status") == "partial_success"
            ]
            if partial_results:
                answer_content = partial_results[-1].get("final_result", "")
            else:
                answer_content = "ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        # Answer Agent ìž…ë ¥ êµ¬ì„±
        answer_input = {
            "agent_type": "hybrid_execution",
            "answer_content": answer_content,
            "execution_results": execution_results,
            "reasoning_trace": self.reasoning_trace_logger.get_global_trace(),
            "trace_summary": trace_analysis.get("trace_summary", {}),
            "overall_confidence": trace_analysis.get("final_assessment", {}).get(
                "confidence", 0.5
            ),
            "source_type": "hybrid_react",
        }

        return self.answer_agent(answer_input)

    def _create_error_response(
        self, error_message: str, execution_time: float
    ) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±"""
        return {
            "final_answer": f"ì‹œìŠ¤í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message}\n\ní•˜ì´ë¸Œë¦¬ë“œ AI ì‹œìŠ¤í…œì´ ë³µêµ¬ë¥¼ ì‹œë„í–ˆì§€ë§Œ ì™„ì „í•œ ì²˜ë¦¬ê°€ ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
            "timestamp": self._get_timestamp(),
            "hybrid_execution_summary": {
                "total_execution_time": f"{execution_time:.2f}ì´ˆ",
                "steps_executed": 0,
                "successful_steps": 0,
                "reasoning_depth": "error",
                "overall_confidence": 0.0,
            },
            "mcp_context": {
                **self.mcp_context,
                "status": "error",
                "error_message": error_message,
                "hybrid_mode_used": False,
                "total_time": execution_time,
            },
        }

    def get_system_status(self) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸

        Returns:
            Dict[str, Any]: ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
        """
        # MCP ë„êµ¬ ìƒíƒœ í™•ì¸
        mcp_tools_status = "unavailable"
        try:
            if self.mcp_multi_client:
                # ë¹„ë™ê¸° ë„êµ¬ ìƒíƒœ í™•ì¸
                async def check_mcp_status():
                    try:
                        tools = await self._get_mcp_tools()
                        return "available" if len(tools) > 0 else "empty"
                    except:
                        return "error"

                mcp_tools_status = self._run_async_mcp_operation(check_mcp_status())
            else:
                mcp_tools_status = "not_initialized"
        except Exception as e:
            mcp_tools_status = f"error: {str(e)}"

        return {
            "orchestrator": "hybrid_running",
            "agents": {
                "router": "initialized",
                "enhanced_planner": "initialized",
                "answer": "enhanced",
                "trace_manager": "initialized",
            },
            "react_executors": {
                "pool_size": len(self.executor_pool),
                "max_executors": self.max_executors,
                "active_executors": list(self.executor_pool.keys()),
            },
            "tools": {
                "reasoning_trace_logger": "active",
                "plan_revision_tool": "active",
                "state_manager": "active",
                "slide_generator_langchain": "available",
                "mcp_tools": mcp_tools_status,
            },
            "mcp_integration": {
                "multi_client_initialized": self.mcp_multi_client is not None,
                "legacy_client_available": self.mcp_client is not None,
                "tools_status": mcp_tools_status,
            },
            "hybrid_features": {
                "parallel_execution": False,  # í–¥í›„ êµ¬í˜„
                "react_reasoning": True,
                "failure_recovery": True,
                "trace_analysis": True,
                "streaming_support": True,
            },
            "mcp_context": self.mcp_context,
        }

    def clear_execution_state(self):
        """ì‹¤í–‰ ìƒíƒœ ì´ˆê¸°í™”"""
        self.executor_pool.clear()
        self.reasoning_trace_logger.clear_traces()
        self.plan_revision_tool.clear_history()
        self.state_manager.clear_all_states()
        print("ðŸ§¹ í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")
