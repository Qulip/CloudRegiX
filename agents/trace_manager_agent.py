from typing import Dict, Any, List
from core.base_agent import BaseAgent
from tools import ReasoningTraceLogger, PlanRevisionTool, StateManager
import json
import re
from datetime import datetime
import logging

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


class TraceManagerAgent(BaseAgent):
    """
    ì „ì²´ ì¶”ë¡  ê³¼ì • ê¸°ë¡ ë° ê´€ë¦¬ Agent
    - ê° ReActExecutorì˜ reasoning trace ìˆ˜ì§‘
    - ì‹¤íŒ¨ ì§€ì  ì‹ë³„ ë° ì¬ì‹œë„ ë¡œì§
    - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    """

    def __init__(self):
        super().__init__("TraceManagerAgent")
        self.trace_logger = ReasoningTraceLogger()
        self.plan_revision_tool = PlanRevisionTool()
        self.state_manager = StateManager()
        self.mcp_context = {
            "role": "trace_manager",
            "function": "reasoning_trace_management",
        }

    def preprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì¶”ë¡  ê³¼ì • ê´€ë¦¬ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
        """
        # ì „ì²´ ì‹¤í–‰ ìƒíƒœ í™•ì¸
        execution_status = self.state_manager.get_execution_status()
        inputs["execution_status"] = execution_status

        # ê¸€ë¡œë²Œ ì¶”ë¡  trace ìˆ˜ì§‘
        global_trace = self.trace_logger.get_global_trace()
        inputs["global_trace"] = global_trace

        return inputs

    def _create_prompt(self, inputs: Dict[str, Any]) -> str:
        """
        ì¶”ë¡  ê³¼ì • ë¶„ì„ ë° ê´€ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        """
        execution_results = inputs.get("execution_results", [])
        failed_steps = inputs.get("failed_steps", [])
        global_trace = inputs.get("global_trace", [])
        execution_status = inputs.get("execution_status", {})

        # ì‹¤íŒ¨í•œ ë‹¨ê³„ë“¤ ìš”ì•½
        failure_summary = ""
        if failed_steps:
            failure_summary = "\n**ì‹¤íŒ¨í•œ ë‹¨ê³„ë“¤:**\n"
            for step in failed_steps:
                failure_summary += f"- {step.get('step_id', 'unknown')}: {step.get('error', 'Unknown error')}\n"

        # ì¶”ë¡  ê³¼ì • ìš”ì•½
        trace_summary = ""
        if global_trace:
            total_thoughts = len(
                [t for t in global_trace if t["step_type"] == "thought"]
            )
            total_actions = len([t for t in global_trace if t["step_type"] == "action"])
            total_observations = len(
                [t for t in global_trace if t["step_type"] == "observation"]
            )

            trace_summary = f"""
**ì¶”ë¡  ê³¼ì • í†µê³„:**
- ì´ Thought ë‹¨ê³„: {total_thoughts}
- ì´ Action ë‹¨ê³„: {total_actions}
- ì´ Observation ë‹¨ê³„: {total_observations}
- ì „ì²´ ì¶”ë¡  ë‹¨ê³„: {len(global_trace)}
"""

        prompt = f"""
ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ê±°ë²„ë„ŒìŠ¤ AI ì‹œìŠ¤í…œì˜ Trace Manager Agentì…ë‹ˆë‹¤.
ì „ì²´ ì›Œí¬í”Œë¡œìš°ì˜ ì¶”ë¡  ê³¼ì •ì„ ë¶„ì„í•˜ê³  ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.

**ì‹¤í–‰ ìƒíƒœ:**
- ì´ Agent ìˆ˜: {execution_status.get('total_agents', 0)}
- ì™„ë£Œëœ Agent: {execution_status.get('completed_agents', [])}
- ì‹¤íŒ¨í•œ Agent: {execution_status.get('failed_agents', [])}
- í™œì„± Agent: {execution_status.get('active_agents', [])}

{trace_summary}

{failure_summary}

**ë¶„ì„ ëª©í‘œ:**
1. ì „ì²´ ì¶”ë¡  ê³¼ì •ì˜ í’ˆì§ˆ í‰ê°€
2. ì‹¤íŒ¨ ì§€ì  ì‹ë³„ ë° ì›ì¸ ë¶„ì„
3. ê°œì„  ê°€ëŠ¥í•œ ë¶€ë¶„ ì œì•ˆ
4. ì¬ì‹œë„ í•„ìš” ì—¬ë¶€ íŒë‹¨
5. ìµœì¢… ì„±ê³µ ê°€ëŠ¥ì„± ì˜ˆì¸¡

**ì¶œë ¥ í˜•ì‹ (JSON):**
{{
    "trace_analysis": {{
        "overall_quality": "excellent|good|fair|poor",
        "reasoning_coherence": 0.0-1.0,
        "goal_achievement_rate": 0.0-1.0,
        "efficiency_score": 0.0-1.0
    }},
    "failure_analysis": {{
        "has_failures": true/false,
        "failure_count": 0,
        "critical_failures": ["step_id1", "step_id2"],
        "failure_patterns": ["pattern1", "pattern2"],
        "root_causes": ["cause1", "cause2"]
    }},
    "recommendations": {{
        "retry_needed": true/false,
        "revision_type": "retry|modify|add_step|replan|none",
        "priority_actions": ["action1", "action2"],
        "expected_improvement": 0.0-1.0
    }},
    "performance_metrics": {{
        "total_execution_time": "ì¶”ì • ì‹œê°„",
        "resource_efficiency": 0.0-1.0,
        "success_probability": 0.0-1.0,
        "reasoning_depth": "shallow|medium|deep"
    }},
    "final_assessment": {{
        "workflow_status": "success|partial_success|failure|needs_revision",
        "confidence": 0.0-1.0,
        "next_action": "complete|retry|revise|abort",
        "summary": "ì „ì²´ ì‹¤í–‰ ê³¼ì • ìš”ì•½"
    }}
}}

ì •í™•í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
"""
        return prompt

    def postprocess(self, outputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì¶”ë¡  ê³¼ì • ë¶„ì„ ê²°ê³¼ í›„ì²˜ë¦¬
        """
        try:
            # LLM ì‘ë‹µì—ì„œ JSON íŒŒì‹±
            content = outputs.content if hasattr(outputs, "content") else str(outputs)
            json_match = re.search(r"\{.*\}", content, re.DOTALL)

            if json_match:
                result = json.loads(json_match.group())

                # ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í›„ì† ì¡°ì¹˜ ê²°ì •
                next_action = result.get("final_assessment", {}).get(
                    "next_action", "complete"
                )

                if next_action == "retry":
                    # ì¬ì‹œë„ ê¶Œì¥ ì‚¬í•­ ìƒì„±
                    retry_plan = self._create_retry_plan(result)
                    result["retry_plan"] = retry_plan

                elif next_action == "revise":
                    # ê³„íš ìˆ˜ì • ê¶Œì¥ ì‚¬í•­ ìƒì„±
                    revision_suggestions = self._create_revision_suggestions(result)
                    result["revision_suggestions"] = revision_suggestions

                # MCP context ì—…ë°ì´íŠ¸
                result["mcp_context"] = {
                    **self.mcp_context,
                    "status": "success",
                    "analysis_completed": True,
                    "next_action": next_action,
                    "workflow_status": result.get("final_assessment", {}).get(
                        "workflow_status", "unknown"
                    ),
                }

                # ìµœì¢… trace ìš”ì•½ ìƒì„±
                result["trace_summary"] = self.trace_logger.get_reasoning_summary()

                return result
            else:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
                return self._create_default_response("JSON íŒŒì‹± ì‹¤íŒ¨")

        except Exception as e:
            return self._create_default_response(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    def _create_retry_plan(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ì¬ì‹œë„ ê³„íš ìƒì„±"""
        failure_analysis = analysis_result.get("failure_analysis", {})
        critical_failures = failure_analysis.get("critical_failures", [])

        return {
            "retry_strategy": "focused_retry",
            "target_steps": critical_failures,
            "max_retry_attempts": 2,
            "retry_modifications": [
                "Increase timeout for long-running operations",
                "Use alternative tools for failed operations",
                "Add more detailed error handling",
            ],
            "success_criteria": {
                "min_success_rate": 0.8,
                "max_execution_time": "5 minutes",
            },
        }

    def _create_revision_suggestions(
        self, analysis_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê³„íš ìˆ˜ì • ì œì•ˆ ìƒì„±"""
        recommendations = analysis_result.get("recommendations", {})
        revision_type = recommendations.get("revision_type", "modify")

        return {
            "revision_type": revision_type,
            "suggested_changes": [
                "Simplify complex steps into smaller sub-steps",
                "Add validation checkpoints between major steps",
                "Implement fallback mechanisms for critical operations",
            ],
            "priority_order": ["high", "medium", "low"],
            "implementation_effort": "medium",
            "expected_benefit": recommendations.get("expected_improvement", 0.5),
        }

    def _create_default_response(self, error_message: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±"""
        return {
            "trace_analysis": {
                "overall_quality": "poor",
                "reasoning_coherence": 0.0,
                "goal_achievement_rate": 0.0,
                "efficiency_score": 0.0,
            },
            "failure_analysis": {
                "has_failures": True,
                "failure_count": 1,
                "critical_failures": ["trace_analysis"],
                "failure_patterns": ["analysis_error"],
                "root_causes": [error_message],
            },
            "recommendations": {
                "retry_needed": True,
                "revision_type": "retry",
                "priority_actions": ["restart_analysis"],
                "expected_improvement": 0.5,
            },
            "final_assessment": {
                "workflow_status": "failure",
                "confidence": 0.1,
                "next_action": "retry",
                "summary": f"ì¶”ë¡  ê³¼ì • ë¶„ì„ ì‹¤íŒ¨: {error_message}",
            },
            "mcp_context": {
                **self.mcp_context,
                "status": "error",
                "message": error_message,
            },
        }

    def handle_failure_recovery(
        self, failed_step: Dict[str, Any], context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬"""
        # ì‹¤íŒ¨ ë‹¨ê³„ ë¶„ì„
        failure_reason = failed_step.get("error", "Unknown error")
        step_id = failed_step.get("step_id", "unknown")

        logger.info(f"ğŸ”„ ì‹¤íŒ¨ ë³µêµ¬ ì²˜ë¦¬ ì¤‘: {step_id}")

        # ê³„íš ìˆ˜ì • ë„êµ¬ ì‚¬ìš©
        revision_result = self.plan_revision_tool.run(
            {
                "current_plan": context.get("execution_plan", []),
                "failed_step": failed_step,
                "failure_reason": failure_reason,
                "context": context,
                "revision_type": "modify",  # ê¸°ë³¸ ìˆ˜ì • ì „ëµ
            }
        )

        if revision_result.get("status") == "success":
            logger.info(f"âœ… ê³„íš ìˆ˜ì • ì™„ë£Œ: {revision_result.get('revision_type')}")
            return {
                "recovery_status": "success",
                "revised_plan": revision_result.get("revised_plan", []),
                "recovery_strategy": revision_result.get("revision_type"),
                "changes_made": revision_result.get("changes_made", {}),
            }
        else:
            logger.error(
                f"âŒ ê³„íš ìˆ˜ì • ì‹¤íŒ¨: {revision_result.get('message', 'Unknown error')}"
            )
            return {
                "recovery_status": "failed",
                "error": revision_result.get("message", "Recovery failed"),
                "fallback_needed": True,
            }

    def get_comprehensive_report(self) -> Dict[str, Any]:
        """ì¢…í•© ì‹¤í–‰ ë³´ê³ ì„œ ìƒì„±"""
        return {
            "execution_summary": self.state_manager.get_execution_status(),
            "reasoning_summary": self.trace_logger.get_reasoning_summary(),
            "revision_history": self.plan_revision_tool.get_revision_history(),
            "performance_metrics": {
                "total_agents": len(self.state_manager.agent_states),
                "successful_completions": len(
                    [
                        agent
                        for agent, state in self.state_manager.agent_states.items()
                        if state.get("current_status") == "completed"
                    ]
                ),
                "total_reasoning_steps": len(self.trace_logger.global_trace),
                "revision_count": len(self.plan_revision_tool.revision_history),
            },
            "recommendations_for_future": [
                "ì¶”ë¡  ê³¼ì •ì—ì„œ ìì£¼ ì‹¤íŒ¨í•˜ëŠ” íŒ¨í„´ ê°œì„ ",
                "ë„êµ¬ ì„ íƒ ë¡œì§ ìµœì í™”",
                "ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ ê°•í™”",
            ],
        }
